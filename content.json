{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://c89757.gitee.io/colinstar","root":"/colinstar/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-12-16T13:28:00.710Z","updated":"2021-12-16T13:28:00.710Z","comments":false,"path":"/404.html","permalink":"http://c89757.gitee.io/colinstar/404.html","excerpt":"","text":""},{"title":"分类","date":"2021-12-16T13:28:00.713Z","updated":"2021-12-16T13:28:00.713Z","comments":false,"path":"categories/index.html","permalink":"http://c89757.gitee.io/colinstar/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-12-16T13:28:00.713Z","updated":"2021-12-16T13:28:00.713Z","comments":true,"path":"links/index.html","permalink":"http://c89757.gitee.io/colinstar/links/index.html","excerpt":"","text":""},{"title":"书单","date":"2021-12-16T13:28:00.712Z","updated":"2021-12-16T13:28:00.712Z","comments":false,"path":"books/index.html","permalink":"http://c89757.gitee.io/colinstar/books/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-12-16T13:28:00.714Z","updated":"2021-12-16T13:28:00.714Z","comments":false,"path":"repository/index.html","permalink":"http://c89757.gitee.io/colinstar/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-12-16T13:28:00.715Z","updated":"2021-12-16T13:28:00.715Z","comments":false,"path":"tags/index.html","permalink":"http://c89757.gitee.io/colinstar/tags/index.html","excerpt":"","text":""},{"title":"关于","date":"2021-12-16T13:28:00.712Z","updated":"2021-12-16T13:28:00.712Z","comments":false,"path":"about/index.html","permalink":"http://c89757.gitee.io/colinstar/about/index.html","excerpt":"","text":"个人详细介绍"}],"posts":[{"title":"Netty","slug":"Netty","date":"2022-01-06T12:58:22.000Z","updated":"2022-01-06T13:17:29.552Z","comments":true,"path":"2022/01/06/Netty/","link":"","permalink":"http://c89757.gitee.io/colinstar/2022/01/06/Netty/","excerpt":"","text":"BIO&amp;NIO&amp;AIOBIO ​ blocking I/O , 即阻塞IO，同步阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时，服务端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，可以通过线程池机制改善（实现多个客户连接服务器） 先看单线程的版本 12345678910111213141516public static void main(String[] args) throws IOException &#123; final ServerSocket serverSocket = new ServerSocket(9000); while (true)&#123; log.info(&quot;等待连接....&quot;); final Socket socket = serverSocket.accept(); log.info(&quot;建立连接&quot;); InputStream inputStream = socket.getInputStream(); byte[] bytes = new byte[1024]; int read = inputStream.read(bytes); if (read != -1)&#123; log.info(&quot;收到消息：&#123;&#125;&quot;,new String(bytes,0,read)); &#125; socket.getOutputStream().write(&quot;已成功接收到消息&quot;.getBytes()); socket.getOutputStream().flush(); &#125; &#125; Debug启动程序，在serverSocket.accept()处打上断点，同时再下一行处也打上断点，然后我们点击idea调试的Resume Program按钮，让程序直接走完；我们会发现断点没有到达下一行，程序也没有停止，而是阻塞在了accept()里。 我们试着用telnet工具去连接程序 按下回车连接的同时，我们也会发现程序的断点跑到了下一行 我们再在 int read = inputStream.read(bytes);这一行及其下一行也打上断点； 程序来到inputStream.read(bytes)这一行，我们再次选择放掉这一个断点，发现此处程序也并没有来到下一行，也是在此处进行了阻塞 我们用telnet工具给服务端发送消息 回到程序，发现程序执行到了下一行 接下来我们重新开始，重新启动服务端，开启一个telnet（客户端1）去连接，但是不发送消息，让程序阻塞在int read = inputStream.read(bytes)这一行；与此同时，我们再另外开启一个telnet客户端（客户端2）去进行连接，然后发送消息给服务端 但是我们发现，控制台并没有任何消息打印； 我们此时在用客户端1去发送消息 发现客户端打印消息，但是打印hello2之前，输出了”建立连接“；说明此时我们其实客户端2并没有真正的连接上，而是阻塞在了serverSocket.accept()处 12final Socket socket = serverSocket.accept();log.info(&quot;建立连接&quot;); 在同一时刻，服务端只能响应一个客户端 解决方案我们可以在将代码改成多线程版本 12345678910111213141516171819202122232425262728293031323334353637383940414243@Slf4jpublic class TestBIO extends Thread&#123; private Socket socket; public TestBIO(Socket socket) &#123; this.socket = socket; &#125; public static void main(String[] args) throws IOException &#123; final ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5000, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); log.info(&quot;等待连接....&quot;); final ServerSocket serverSocket = new ServerSocket(9000); while (true)&#123; Socket socket = serverSocket.accept(); log.info(&quot;建立连接&quot;); threadPoolExecutor.execute(new TestBIO(socket)); if (false)&#123; break;&#125; &#125; &#125; public static void handler(Socket socket) throws IOException &#123; InputStream inputStream = socket.getInputStream(); byte[] bytes = new byte[1024]; int read = inputStream.read(bytes); if (read != -1)&#123; log.info(&quot;收到消息：&#123;&#125;&quot;,new String(bytes,0,read)); &#125; socket.getOutputStream().write(&quot;已成功接收到消息&quot;.getBytes()); socket.getOutputStream().flush(); &#125; @Override public void run() &#123; try &#123; handler(this.socket); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 我们再实验以上步骤 先开启telnet客户端1去连接阻塞，但是不发送消息 再开启telnet客户端2去连接，并发送消息 结果此次控制台能正确接收到消息 存在的问题​ 如果开辟大量线程，比较消耗资源，且如果我们用了线程池，如果我们线程池数量是500，某一瞬间并发量有1w，那后面的请求就只能阻塞等待。又或者500线程池，其中400个线程只是和你建立连接，并不立马发送消息给服务端，那这个线程会一直被这个连接给占用，其他人无法获取; 又或者用完线程给别人用时，线程的切换也是比较消耗资源的 IO代码里read操作是阻塞操作，如果连接不做数据读写会导致线程阻塞，浪费资源 如果线程很多，会导致服务器线程太大，压力太大 应用场景：BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高 NIO​ Non Blocking IO,或者读为New IO,同步非阻塞，服务器实现模式为一个线程可以处理多个请求（连接），客户端发送的连接请求都会注册到多路复用器selector上，多路复用器轮询到连接有IO请求就进行处理，JDK1.4开始引入 123456789101112131415161718192021222324252627282930313233343536373839@Slf4jpublic class TestNIO &#123; private static List&lt;SocketChannel&gt; channelList = new ArrayList&lt;&gt;(); public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(9000)); // 设置ServerSocketChannel为非阻塞 serverSocketChannel.configureBlocking(false); while (true) &#123; // 非阻塞模式accept方法不会阻塞，否则会阻塞 // NIO的非阻塞是由操作系统内部实现的，底层调用了linux内核的accept函数 SocketChannel socketChannel = serverSocketChannel.accept(); if (socketChannel != null) &#123; // 如果有客户端进行连接 log.info(&quot;连接成功&quot;); // 设置SocketChannel为非阻塞 socketChannel.configureBlocking(false); // 保存客户端连接在list中 channelList.add(socketChannel); &#125; // 遍历连接进行数据读取 Iterator&lt;SocketChannel&gt; iterator = channelList.iterator(); while (iterator.hasNext()) &#123; SocketChannel next = iterator.next(); ByteBuffer byteBuffer = ByteBuffer.allocate(128); // 非阻塞模式read方法不会阻塞 int len = next.read(byteBuffer); if (len &gt; 0) &#123; log.info(&quot;接收到消息: &#123;&#125;&quot;, new String(byteBuffer.array())); &#125; else if (len == -1) &#123; // 如果客户端断开，把socket从集合中删调 iterator.remove(); log.info(&quot;与客户端断开连接&quot;); &#125; &#125; &#125; &#125;&#125; 我们先后开启两个telnet客户端去连接服务端，发送消息，服务端都能接收到; 会一直循环去判断是否有新的连接请求，是否有连接发送消息 上述代码存在的问题： 如果连接数太多的话，会有大量的无效遍历 比如如果我现在有10万个连接，但是经常给服务端发消息的就那个几百个，但是每次都要去遍历所有的连接 我们可以将那些有数据交互的连接，存储在另外一个数据结构中，每次遍历只需要遍历那些有数据交互的连接 NIO引入多路复用器代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Slf4jpublic class NioSelectorServer &#123; public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(9000)); // 设置ServerSocketChannel为非阻塞 serverSocketChannel.configureBlocking(false); // 打开selector处理Channel，即创建epoll Selector selector = Selector.open(); // 把ServerSocketChannel注册到selector上，并且selector监听客户端accept连接事件 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; // 阻塞等待需要处理的事件发生 selector.select(); // 获取selector中注册的全部事件的SelectionKey实例 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); // 遍历SelectionKey对事件进行处理 while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); // 如果是OP_ACCEPT事件，则进行连接获取和事件注册 if (key.isAcceptable())&#123; ServerSocketChannel channel =(ServerSocketChannel) key.channel(); final SocketChannel socketChannel = channel.accept(); socketChannel.configureBlocking(false); // 这里只注册了读事件，如果需要给客户端发送数据可以注册写事件 socketChannel.register(selector,SelectionKey.OP_READ); log.info(&quot;客户端连接成功&quot;); &#125;else if(key.isReadable())&#123; // 如果是OP_READ事件，则进行读取和打印 SocketChannel channel =(SocketChannel)key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); int len = channel.read(byteBuffer); if (len &gt;0 )&#123; log.info(&quot;接收到消息:&#123;&#125;&quot;,new String(byteBuffer.array())); &#125;else if(len == -1)&#123; log.info(&quot;客户端断开连接&quot;); channel.close(); &#125; &#125; // 从事件集合里删除本次处理的key,防止下次select重复处理 iterator.remove(); &#125; &#125; &#125;&#125; NIO有三大核心组件：Channel(通道)，Buffer(缓冲区)，Selector(多路复用器) 1、channel类似于流，每个channel对应一个buffer缓冲区，buffer底层就是个数组 2、channel会注册到selector上，由selector根据channel独写事件的发生将其交由某个空闲的线程处理 3、NIO的Buffer和channe都是既可以读也可以写 先看一幅图 ​ 我们代码最开始处，创建了一个ServerSocketChannel，并绑定9000端口,并将ServerSocketChannel注册到selector上，并且selector监听客户端accept连接事件，注册上后会返回一个key,通过这个selectionKey可以找到与之绑定的ServerSocketChannel; ​ 我们在selector.select()处及其下一行打上断点，启动项目。 ​ 放掉断点让其走完，发现程序阻塞在了这一行； ​ 同样的，打开cmd，用telnet连接 1telnet localhost 9000 ​ ​ 连接上后，发现程序走到了下一行 ​ 继续往下走一行，获取到所有的selectionKey; 因为此时我们只有一个客户端进行连接，所以此处size是1 ​ 很显然我们此处是OP_ACCEPT事件 ​ 通过selectionKey可以拿到与之绑定的ServerSocketChannel，并让其与客户端建立连接,并把客户端对应的socketChannel也注册到selector上，并让其监听读事件（读是相当于服务端来的，也就是监听客户端发送过来的消息） ​ 我们一步一步调试，让程序走完，因为是死循环，在select处又会进行阻塞，因为此时既没有新的客户端连接进来，刚刚连接上的客户端也没有发送消息。 ​ 我们用telnet再给服务端发送一条消息 ​ 此时，程序停止了阻塞，走到了下一行 ​ 一步一步调试，很显然这次我们是OP_READ事件，通过key拿到与客户端对应的SocketChannel。也就是下图标识出来的部分，用它来读取客户端的数据 ​ 我们现在再另外开启一个telnet客户端，连接服务端 ​ 我们可以看到，现在有两个客户端，但是拿到的selectionKey只有一个，只针对那些发生的事件进行处理 ​ NIO底层在JDK1.4版本是用linux的内核函数select()或poll()来实现，跟上面最开始的代码类似，selector每次都会轮询所有的socketChannel看下哪个channel有读写事件，有的话就处理，没有就继续遍历，JDK1.5引入了epoll基于事件响应机制来优化NIO 几个核心APISelector.open();1Selector selector = Selector.open(); provider()方法里最终调用了下面的create()方法，发现其new 了一个WindowsSelectorProvider()。因为我们日常使用的是windows的jdk 123public static Selector open() throws IOException &#123; return SelectorProvider.provider().openSelector();&#125; 12345678public class DefaultSelectorProvider &#123; private DefaultSelectorProvider() &#123; &#125; public static SelectorProvider create() &#123; return new WindowsSelectorProvider(); &#125;&#125; 下载openJdk8u的源码，搜索DefaultSelectorProvider这个类，发现有三个，分别对应unix系统，mac系统，windows系统。我们接下来看unix系统对应的源码 unix系统create()方法的源码如下，发现和windows的有区别，如果是linux系统，会返回EPollSelectorProvider这个类 123456789public static SelectorProvider create() &#123; String osname = AccessController .doPrivileged(new GetPropertyAction(&quot;os.name&quot;)); if (osname.equals(&quot;SunOS&quot;)) return createProvider(&quot;sun.nio.ch.DevPollSelectorProvider&quot;); if (osname.equals(&quot;Linux&quot;)) return createProvider(&quot;sun.nio.ch.EPollSelectorProvider&quot;); return new sun.nio.ch.PollSelectorProvider();&#125; open()方法里会调用openSelector()这个方法，EPollSelectorProvider里的实现如下，直接new 了一个EPollSelectorImpl 1234567891011public class EPollSelectorProvider extends SelectorProviderImpl&#123; public AbstractSelector openSelector() throws IOException &#123; return new EPollSelectorImpl(this); &#125; public Channel inheritedChannel() throws IOException &#123; return InheritedChannel.getChannel(); &#125;&#125; 接着我们去看看EpollSelectorImpl这个类的构造函数，初始化的时候， new EPollArrayWrapper()创建了一个EPollArrayWrapper对象 123456789EPollSelectorImpl(SelectorProvider sp) throws IOException &#123; super(sp); long pipeFds = IOUtil.makePipe(false); fd0 = (int) (pipeFds &gt;&gt;&gt; 32); fd1 = (int) pipeFds; pollWrapper = new EPollArrayWrapper(); pollWrapper.initInterrupt(fd0, fd1); fdToKey = new HashMap&lt;&gt;();&#125; 紧接着我们看到EPollArrayWrapper的构造函数，里面调用了一个epollCreate（）方法 12345678910111213EPollArrayWrapper() throws IOException &#123; // creates the epoll file descriptor epfd = epollCreate(); // the epoll_event array passed to epoll_wait int allocationSize = NUM_EPOLLEVENTS * SIZE_EPOLLEVENT; pollArray = new AllocatedNativeObject(allocationSize, true); pollArrayAddress = pollArray.address(); // eventHigh needed when using file descriptors &gt; 64k if (OPEN_MAX &gt; MAX_UPDATE_ARRAY_SIZE) eventsHigh = new HashMap&lt;&gt;();&#125; ​ epollCreate是一个本地方法 （java的native方法是通过JNI，即java native interface来实现的，可以通过它来实现java与其他语言之间的交互） 1private native int epollCreate(); EPollArrayWrapper.c里找到这个epollCreate方法, epoll_create是linux的一个系统函数 12345678910111213JNIEXPORT jint JNICALLJava_sun_nio_ch_EPollArrayWrapper_epollCreate(JNIEnv *env, jobject this)&#123; /* * epoll_create expects a size as a hint to the kernel about how to * dimension internal structures. We can&#x27;t predict the size in advance. */ int epfd = epoll_create(256); if (epfd &lt; 0) &#123; JNU_ThrowIOExceptionWithLastError(env, &quot;epoll_create failed&quot;); &#125; return epfd;&#125; 我们在linux系统上执行 man epoll_create命令，查看这个函数的文档 -打开一个文件描述符，相当于创建了一个epoll对象，返回文件描述符的索引 int epfd = epoll_create(256) serverSocketChannel.register(…)1serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); java.nio.channels.SelectableChannel#register(java.nio.channels.Selector, int) 12345public final SelectionKey register(Selector sel, int ops) throws ClosedChannelException&#123; return register(sel, ops, null);&#125; java.nio.channels.spi.AbstractSelectableChannel#register 这个方法里面最终又调用了一个register方法，我们再点进去 12345678910111213141516public final SelectionKey register(Selector sel, int ops, Object att) throws ClosedChannelException&#123; synchronized (regLock) &#123; ..... synchronized (keyLock) &#123; if (!isOpen()) throw new ClosedChannelException(); k = ((AbstractSelector)sel).register(this, ops, att); // 主要看这个register方法 addKey(k); &#125; &#125; ..... &#125;&#125; sun.nio.ch.SelectorImpl#register 这个里面又调用了一个implRegister（）方法，我们点进去是个抽象方法 protected abstract void implRegister(SelectionKeyImpl var1); 查看他的实现类，来到了WindowsSelectorImpl，这是windows系统的实现，我们去查看linux的implRegister的实现方法 123456789protected final SelectionKey register(AbstractSelectableChannel var1, int var2, Object var3) &#123; .... synchronized(this.publicKeys) &#123; this.implRegister(var4); &#125; .... &#125;&#125; sun.nio.ch.EPollSelectorImpl#implRegister pollWrapper.add(fd); fd是文件描述符，会根据这个索引找到这个文件（linux一切皆文件），在此处就是linux系统能够根据pd这个文件描述符找到这个创建好的serverSocketChannel； 这个pollWrapper就是上面Selector.open()里创建的pollWrapper 123456789protected void implRegister(SelectionKeyImpl ski) &#123; if (closed) throw new ClosedSelectorException(); SelChImpl ch = ski.channel; int fd = Integer.valueOf(ch.getFDVal()); fdToKey.put(fd, ski); pollWrapper.add(fd); keys.add(ski);&#125; selector.select();123Selector selector = Selector.open(); ....selector.select(); select是一个抽象方法 1public abstract int select() throws IOException; 点进实现类 sun.nio.ch.SelectorImpl#select() 123public int select() throws IOException &#123; return this.select(0L);&#125; lockAndDoSelect 1234567public int select(long var1) throws IOException &#123; if (var1 &lt; 0L) &#123; throw new IllegalArgumentException(&quot;Negative timeout&quot;); &#125; else &#123; return this.lockAndDoSelect(var1 == 0L ? -1L : var1); &#125;&#125; sun.nio.ch.SelectorImpl#lockAndDoSelect 123456789101112private int lockAndDoSelect(long var1) throws IOException &#123; ...... synchronized(this.publicKeys) &#123; synchronized(this.publicSelectedKeys) &#123; var10000 = this.doSelect(var1); &#125; &#125; ...... &#125; &#125;&#125; doSelect是一个抽象方法，点进实现类来到了WindowsSelectorImpl。同样的，我们需要看linux的实现EPollSelectorImpl 1protected abstract int doSelect(long var1) throws IOException; sun.nio.ch.EPollSelectorImpl#doSelect 1234567protected int doSelect(long timeout) throws IOException &#123; ....... pollWrapper.poll(timeout); ....... &#125; return numKeysUpdated;&#125; sun.nio.ch.EPollArrayWrapper#poll 123456789101112int poll(long timeout) throws IOException &#123; updateRegistrations(); updated = epollWait(pollArrayAddress, NUM_EPOLLEVENTS, timeout, epfd); for (int i=0; i&lt;updated; i++) &#123; if (getDescriptor(i) == incomingInterruptFD) &#123; interruptedIndex = i; interrupted = true; break; &#125; &#125; return updated;&#125; 先看updateRegistrations（）方法 updateRegistrations(); 此方法里又调用了一个epollCtl(epfd, opcode, fd, events) ，点进去，这是一个本地方法 private native void epollCtl(int epfd, int opcode, int fd, int events); 内部调用的就是linux函数epoll_ctl 123456789101112131415161718192021222324252627282930private void updateRegistrations() &#123; synchronized (updateLock) &#123; int j = 0; while (j &lt; updateCount) &#123; int fd = updateDescriptors[j]; short events = getUpdateEvents(fd); boolean isRegistered = registered.get(fd); int opcode = 0; if (events != KILLED) &#123; if (isRegistered) &#123; opcode = (events != 0) ? EPOLL_CTL_MOD : EPOLL_CTL_DEL; &#125; else &#123; opcode = (events != 0) ? EPOLL_CTL_ADD : 0; &#125; if (opcode != 0) &#123; epollCtl(epfd, opcode, fd, events); if (opcode == EPOLL_CTL_ADD) &#123; registered.set(fd); &#125; else if (opcode == EPOLL_CTL_DEL) &#123; registered.clear(fd); &#125; &#125; &#125; j++; &#125; updateCount = 0; &#125; &#125; 在linux系统上执行命令 1man epoll_ctl 查看此函数 epollCtl(epfd, opcode, fd, events); epfd epoll实例对应的文件描述符 fd socketChannel对应的文件描述符events 事件 参数opcode又以下几个值： 123EPOLL_CTL_ADD // 注册新的SocketChannel到epoll实例中，并关联事件eventEPOLL_CTL_DEL // 修改已经注册的SocketChannel的监听事件EPOLL_CTL_MOD // 从epoll中移除SocketChannel，并且忽略掉绑定的event epollCtl这个方法把SocketChannel和epoll关联起来 2.updated = epollWait(pollArrayAddress, NUM_EPOLLEVENTS, timeout, epfd); 再回到poll方法里，程序继续往下走，接着看epollWait这个方法，点进去也是一个本地方法，也是调用的操作系统内核函数 epoll_wait epoll_wait, epoll_pwait - wait for an I/O event on an epoll file descriptor epoll_wait的时候，会去查看sector里面的rdlist就绪列表里是否有数据，有数据就跳出阻塞，没有就阻塞住 利用操作系统回调函数，客户端有响应，把事件放进rdlist AIO（NIO 2.0）​ 异步非阻塞，由操作系统完成后回调通知服务端程序启动线程去处理，一般适用于连接数较多并且连接时间较长的应用 代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940public class TestAIO &#123; public static void main(String[] args) throws IOException &#123; AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(9000)); serverSocketChannel.accept(null, new CompletionHandler&lt;AsynchronousSocketChannel, Object&gt;() &#123; @Override public void completed(AsynchronousSocketChannel socketChannel, Object attachment) &#123; try &#123; System.out.println(&quot;2----&quot; + Thread.currentThread().getName()); // 再此接收客户端连接,如果不写这行代码后面的客户端连接不上服务端 serverSocketChannel.accept(attachment, this); System.out.println(socketChannel.getRemoteAddress()); ByteBuffer buffer = ByteBuffer.allocate(1024); socketChannel.read(buffer, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; System.out.println(&quot;3---&quot; + Thread.currentThread().getName()); buffer.flip(); System.out.println(new String(buffer.array(), 0, result)); socketChannel.write(ByteBuffer.wrap(&quot;HelloClient&quot;.getBytes())); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; exc.printStackTrace(); &#125; &#125;); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; @Override public void failed(Throwable exc, Object attachment) &#123; exc.printStackTrace(); &#125; &#125;); System.out.println(&quot;1---&quot;+Thread.currentThread().getName()); System.in.read(); &#125;&#125;","categories":[{"name":"nio","slug":"nio","permalink":"http://c89757.gitee.io/colinstar/categories/nio/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://c89757.gitee.io/colinstar/tags/nio/"}]},{"title":"关于count(*)","slug":"关于count()","date":"2021-12-22T12:09:37.000Z","updated":"2021-12-22T13:01:32.776Z","comments":true,"path":"2021/12/22/关于count()/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/22/%E5%85%B3%E4%BA%8Ecount()/","excerpt":"","text":"count(*)的不同实现方式 在 msyql 引擎中，count（*）有不同的实现方式 MyISAM引擎把一个表的总行数存在了磁盘上,因此执行count(*)的时候会直接返回这个数，效率很高 而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累计计数 当然，这里说的是不加where条件的count(*)，如果加了条件，MyISAM表也不能返回这么快的。 为什么InnoDB不像MyISAM一样，也把数字存起来呢？ ​ 因为即使在同一个时刻的多个子查询，由于多版本并发控制（MVCC）的原因，而InnoDB表 应该返回多少行 也是不确定的。 比如现在某表中有1000条数据 会话A去执行select(*) 会话B开启事务，新增一条数据，再执行select * 会话A和会话B在同一时刻执行，那么他们返回的总行数是不一样的，A返回1000，而B返回1001 这和InnoDB的事务有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是MVCC来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB会把数据一行一行的读出来依次判断，可见的行才能够计算“基于这个查询”的表的总行数 ​ MySQL在执行 count(*)操作的时候还是做了优化的。 InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是 主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树 得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑 正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一 解决方案 如果一个页面需要经常查询显示某表的总数，应该如何去做呢？ 我们应该自己去计数 用缓存系统保存计数​ 可以用Redis去记录这个表的总行数。每插入一行Redis计数就加1，每删除一行Redis计数就减1。 可能存在的问题： 1、Redis可能会丢失数据，如果我们刚在表里插入了一行数据，Redis中的值也进行了自增，然后Redis宕机了，还没来得及进行持久化，导致数据的丢失； （我们可以在Redis宕机后，手动select(*)查询总行数写回Redis) 2、Redis和MySql存在分布式事务问题； 比如某个场景下，我们需要查询显示总数，并且还要显示最近操作的100条记录。那我们就需要先从Redis里面取出计数，再去表里取数据记录 可能存在的问题，查到的100行里面没有新增的数据，但Redis的计数已经加1 另一种是，查到的100行有新增的数，但是Redis的计数还没加1 产生的原因就是，无法保证提交数据库事务的同时写入Redis， 在数据库保存计数​ 用一张表去记录总数，可以避免上述问题，因此事务的可见性，我们插入数据和修改表中记录的行数都是在方法执行完后统一提交的事务，事务还未提交时，对其他线程是不可见的 从并发系统性能的角度看，应该先插数据表，还是先更新计数表呢？ 更新计数表会涉及到行锁的竞争，先插入再更新能最大程度的减少了事务之间的锁等待，提高并发度（事务开启后，更新操作放到最后，减少锁等待时间的影响） 不同count的用法count(*)、count(id)、count(字段)、count(1)的用法的性能，有哪些差别呢。 基于InnoDB引擎 count（）是一个聚合函数，对于返回的结果集，一行一行的判断，如果count函数的参数不是null,就会累计值加1，否则不加。 所以count(*),count(id),count(字段),count(1)都返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里，参数“字段”不为null的总个数 对于count(id)来说。InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层，server层拿到id后，判断是不可能为空的，就按行累加 对于count(1)来说。InnoDB引擎遍历整张表，但是不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加 count(*)执行的要比count(id)快，因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作 对于count(字段)来说。 如果这个字段是定义为not null的话，一行行的从记录里面读取出这个字段，判断不能为null,按行累加； 如果这个字段允许为空，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加 对于count(*)来说。并不会把全部字段取出来，而是专门做了优化，不取值，count(*)肯定不是null,按行累加 按照效率排序的话，count(字段) &lt; count(id) &lt; count(1) ≈ count(*)","categories":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/tags/mysql/"}]},{"title":"Sychronized关键字-monitorenter与monitorexit","slug":"Sychronized关键字-monitorenter与monitorexit","date":"2021-12-17T11:19:25.000Z","updated":"2021-12-17T11:47:41.031Z","comments":true,"path":"2021/12/17/Sychronized关键字-monitorenter与monitorexit/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/17/Sychronized%E5%85%B3%E9%94%AE%E5%AD%97-monitorenter%E4%B8%8Emonitorexit/","excerpt":"","text":"每个对象都有一个Monitor与之关联，当Monitor被持有后，它将处于锁定状态。Synchronized在JVM里的实现都是 基于进入和退出Monitor对象来实现方法同步和代码块同步，都可以通过成对的MonitorEnter和MonitorExit指令来实现。 12345public void method() &#123; synchronized(this) &#123; System.out.println(&quot;hello world&quot;); &#125; &#125; 经过javap解析后 1234567891011121314151617181920212223public void method(); Code: 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String hello world 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit 14: goto 22 17: astore_2 18: aload_1 19: monitorexit 20: aload_2 21: athrow 22: return Exception table: from to target type 4 14 17 any 17 20 17 any 此处会发现有一个monitorenter，却有两个monitorexit；这是JVM的补偿机制，保证你的同步代码块中出现异常，能正常释放锁 如字节码行号4-13可能会出现异常，则会走17进行异常处理，在此处进行锁的释放","categories":[{"name":"juc","slug":"juc","permalink":"http://c89757.gitee.io/colinstar/categories/juc/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/tags/JUC/"},{"name":"多线程","slug":"多线程","permalink":"http://c89757.gitee.io/colinstar/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"MySql性能调优","slug":"MySql性能调优","date":"2021-12-16T15:08:30.000Z","updated":"2021-12-17T11:47:30.089Z","comments":true,"path":"2021/12/16/MySql性能调优/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/16/MySql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","excerpt":"","text":"啥也没有，只是为了样式展示","categories":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/tags/mysql/"}]},{"title":"gitee+hexo搭建个人博客","slug":"gitee+hexo搭建个人博客","date":"2021-12-16T12:42:22.000Z","updated":"2021-12-17T11:49:04.429Z","comments":true,"path":"2021/12/16/gitee+hexo搭建个人博客/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/16/gitee+hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"概述事前准备： 先创建一个仓库，同时在仓库根目录下创建index.html (gitee官网这样说的,没试过缺少这个文件会怎样) 安装Hexo所需环境 node.js git 自行进行下载与安装 安装1234567891011# 安装hexonpm install -g hexo# 创建文件夹,用来存储你博客内容hexo init test# cd到创建的目录,执行npm install# 开启hexo服务hexo s 访问http://localhost:4000；没有问题的话就会显示他的默认页面 修改配置关联git仓库，在你创建的目录下找到 config.yml文件（例如此处我的是test/_config.yml） 打开添加如下配置 1234deploy: type: &#x27;git&#x27; repository: https://gitee.com/xxx/xxxx # 你的仓库地址 branch: master # 你的仓库分支 生成静态页面 1hexo g #或者 hexo generate 123456# 此时若出现如下报错：ERROR Local hexo not found in ~/blogERROR Try runing: &#x27;npm install hexo --save&#x27;# 则执行命令：npm install hexo --save 将生成的页面提交到仓库 1hexo d #或者hexo deploy 若执行命令hexo deploy报错：无法连接git或找不到git，则执行如下命令来安装hexo-deployer-git： 1npm install hexo-deployer-git --save 发布文章进入到你创建的“text”目录，新建文章，执行 1hexo new &quot;blog&quot; 此时在test/source/_posts下，会新建一个名为“blog.md”的文件，利用相关markdown编辑器就能编写你的博客啦!(我这里用的typore) 123hexo g # 生成静态页面hexo d # 部署到gitee hexo有许多主题，默认生成的主题都是landscape，你也可以去主题官网寻找自己喜欢的主题 例如主题pure 1234567891011git clone https://github.com/cofess/hexo-theme-pure.git themes/pure#修改test目录下_config.yml里theme的名称,将landscape修改为pure即可 hexo clean#清除缓存文件 (db.json) 和静态文件 (public)hexo g#生成缓存和静态文件hexo d #重新部署到服务器","categories":[{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/categories/others/"}],"tags":[{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/tags/others/"}]}],"categories":[{"name":"nio","slug":"nio","permalink":"http://c89757.gitee.io/colinstar/categories/nio/"},{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/categories/mysql/"},{"name":"juc","slug":"juc","permalink":"http://c89757.gitee.io/colinstar/categories/juc/"},{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/categories/others/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://c89757.gitee.io/colinstar/tags/nio/"},{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/tags/mysql/"},{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/tags/JUC/"},{"name":"多线程","slug":"多线程","permalink":"http://c89757.gitee.io/colinstar/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/tags/others/"}]}