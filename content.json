{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://c89757.gitee.io/colinstar","root":"/colinstar/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-12-16T13:28:00.710Z","updated":"2021-12-16T13:28:00.710Z","comments":false,"path":"/404.html","permalink":"http://c89757.gitee.io/colinstar/404.html","excerpt":"","text":""},{"title":"书单","date":"2021-12-16T13:28:00.712Z","updated":"2021-12-16T13:28:00.712Z","comments":false,"path":"books/index.html","permalink":"http://c89757.gitee.io/colinstar/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-12-16T13:28:00.713Z","updated":"2021-12-16T13:28:00.713Z","comments":false,"path":"categories/index.html","permalink":"http://c89757.gitee.io/colinstar/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-12-16T13:28:00.713Z","updated":"2021-12-16T13:28:00.713Z","comments":true,"path":"links/index.html","permalink":"http://c89757.gitee.io/colinstar/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-12-16T13:28:00.714Z","updated":"2021-12-16T13:28:00.714Z","comments":false,"path":"repository/index.html","permalink":"http://c89757.gitee.io/colinstar/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-12-16T13:28:00.715Z","updated":"2021-12-16T13:28:00.715Z","comments":false,"path":"tags/index.html","permalink":"http://c89757.gitee.io/colinstar/tags/index.html","excerpt":"","text":""},{"title":"关于","date":"2021-12-16T13:28:00.712Z","updated":"2021-12-16T13:28:00.712Z","comments":false,"path":"about/index.html","permalink":"http://c89757.gitee.io/colinstar/about/index.html","excerpt":"","text":"个人详细介绍"}],"posts":[{"title":"maven打包报错:Malformed \\uxxx encoding","slug":"maven打包报错-Malformed-uxxx-encoding","date":"2022-06-09T14:27:51.000Z","updated":"2022-06-09T15:13:40.832Z","comments":true,"path":"2022/06/09/maven打包报错-Malformed-uxxx-encoding/","link":"","permalink":"http://c89757.gitee.io/colinstar/2022/06/09/maven%E6%89%93%E5%8C%85%E6%8A%A5%E9%94%99-Malformed-uxxx-encoding/","excerpt":"","text":"在最近一次项目构建中，执行mvn clean package却报出Malformed \\uxxx encoding的错误。查阅资料后， 给出了以下几种解决方式： 删除~/.m2/repository/path-to-the-library的包 将项目中的/更改为\\ 对我来说都不得行，最后查阅到有人说是resolver-status.properties损坏，里面包含了\\u0000， 然后我就根据Maven提示，带上参数 -e - X 执行命令：Maven -e - X clean package，日志打印出是哪个包报错， 最后来到仓库中，删除对应的resolver-status.properties文件，再次构建成功！ 那resolver-status.properties文件是干嘛的呢？ Maven更新本地仓库的步骤是：先更新元文件，再根据元文件去更新本地仓库jar包 而元文件有三个maven-metadata-local.xml，maven-metadata-snapshot-nexus.xml，resolver-status.properties maven-metadata-local.xml 在本地install代码后会生成该文件，记录的是本地项目编译的时间戳 maven-metadata-snapshot-nexus.xml 从远程仓库拉取jar包后，会同时从仓库下载该元文件，该文件记录的是远程仓库上项目最新版本的时间 resolver-status.properties 从远程仓库拉取jar包的时候，也会生成该文件，并且每次拉取都会更新。该文件主要作用是记录maven-metadata–nexus.xml 文件的上次更新时间戳，并结合标签完成更新策略的一部分 更新本地jar包：依赖于 maven-metadata-local.xml 和 maven-metadata-snapshot-nexus.xml 两个文件 如果只有 maven-metadata-local.xml 文件，一般来说是配置有错，或者并没有从远程仓库中拉取过jar包 如果两个文件都有，每次都需要比较一下两个文件的时间戳，即标签上的时间戳。 如果local.xml的时间戳比snapshot.xml的时间戳要新，就不会从远程仓库下载； 如果local.xml的时间戳比snapshot.xml的时间戳要旧，就会去检查一下本地maven仓库的该项目文件夹路径下是否有snapshot.xml对应版本的jar包 如果没有该版本的jar包，就会从远程仓库拉取该版本的jar包 如果有该版本的jar包，就不会做任何行为 更新本地元文件：更新本地仓库jar包决定于本地元文件 maven-metadata-snapshot-nexus.xml，该文件的更新取决于resolver-status.properties文件 先去远程仓库获取maven-metadata-snapshot-nexus.xml文件，远程仓库中不存在此文件，那么会走下载流程 如果存在，读取resolver-status.properties中的lastUpdated参数，然后与当前的时间做比较，根据跟新策略是否需要下载（always/never/daily…) 附上stackOverflow上的回答：https://stackoverflow.com/questions/68003423/java-lang-illegalargumentexception-malformed-uxxxx-encoding-while-mvn-install","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://c89757.gitee.io/colinstar/tags/maven/"}]},{"title":"restTemplate","slug":"restTemplate","date":"2022-06-01T12:26:08.000Z","updated":"2022-06-01T16:26:14.544Z","comments":true,"path":"2022/06/01/restTemplate/","link":"","permalink":"http://c89757.gitee.io/colinstar/2022/06/01/restTemplate/","excerpt":"","text":"概述 最近项目有个新需求，需要将原来的接口支持https，虽然之前也有用过restTemplate，但一直未对其进行过深入了解，今天便来看一看 官方文档：https://docs.spring.io/spring-framework/docs/current/reference/html/integration.html#rest-client-accessRestTemplate 有两种方法可以创建restTemplate实例，一种是直接new，另一种是通过构建者构建出来 new 12345678// 直接newRestTemplate restTemplate = new RestTemplate();// 也可以调用有参构造，传入一个ClientHttpRequestFactory的实现类SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory();factory.setConnectTimeout(3000);factory.setReadTimeout(3000);RestTemplate restTemplate = new RestTemplate(factory); build 123456RestTemplate restTemplate = new RestTemplateBuilder() .basicAuthentication(&quot;username&quot;, &quot;password&quot;) .setConnectTimeout(Duration.ofMillis(3000)) .setReadTimeout(Duration.ofMillis(3000)) .rootUri(&quot;http://example/base/&quot;) .build(); 常用API postForEntity 12345String url = &quot;http://www.baidu.com&quot;;HttpHeaders header = new HttpHeaders();header.add(&quot;auth&quot;,&quot;bearer ****&quot;);HttpEntity&lt;UserReqInfo&gt; httpEntity = new HttpEntity&lt;&gt;(new UserReqInfo(),header);ResponseEntity&lt;UserRespInfo&gt; orderResponseEntity = restTemplate.postForEntity(url, httpEntity, UserRespInfo.class); 对于基本类型和实体传参，必须使用MultiValueMap传参 ​ 什么是基本类型和实体传参呢？类似于form表单，如下 12@PostMapping(&quot;/test&quot;)public void test(UserDTO userDTO,Integer requestId)&#123;&#125; 而对于@Requestbody传参，需要使用HttpEntity传参 exhange exchange有以下几种重载方法 url：请求路径 method：请求方法 requestEntity：封装请求头和请求体 responseType：返回数据类型 uriVariables：支持PathVariable类型的数据 示例： 12345678910String url = &quot;http://www.baidu.com&quot;;// 创建http的headerHttpHeaders header = new HttpHeaders();header.add(&quot;auth&quot;,&quot;bearer ****&quot;);header.setContentType(MediaType.APPLICATION_JSON);Gson gson = new Gson();String json = gson.toJson(new UserReqInfo()); // UserReqInfo自定义实体类// 设置请求体和请求头HttpEntity&lt;String&gt; httpEntity = new HttpEntity&lt;&gt;(json,header);ResponseEntity&lt;UserRespInfo&gt; exchange = restTemplate.exchange(url, HttpMethod.POST, httpEntity, UserRespInfo.class); // UserRespInfo自定义实体类 execute restTemplate的所有get,post等等方法，最终都是调用的execute方法。 原理初始化 默认使用HttpUrlConnection，可以通过构造方法传入一个ClientHttpRequestFactory的实现类，以此来替换底层的执行引擎，常见的执行引擎包括HttpClient、Netty、OKHttp。 无参构造 调用RestTemplate无参构造初始化时，会去调用父类InterceptingHttpAccessor的无参构造，其又会去调用顶级父类HttpAccessor的无参构造，虽然无参构造啥也没做，但是可以看到，默认的ClientHttpRequestFactory在类加载时已经初始化为SimpleClientHttpRequestFactory了 有参构造 我们可以先创建一个HttpComponentsClientHttpRequestFactory的实例，该类的执行引擎用的是HttpClient exchange（）方法","categories":[],"tags":[]},{"title":"堆排序","slug":"堆排序","date":"2022-03-24T11:44:57.000Z","updated":"2022-03-25T09:00:20.941Z","comments":true,"path":"2022/03/24/堆排序/","link":"","permalink":"http://c89757.gitee.io/colinstar/2022/03/24/%E5%A0%86%E6%8E%92%E5%BA%8F/","excerpt":"","text":"​ 堆排序是利用堆这种数据结构而设计的一种排序算法，堆排序是一种选择排序，它的最好，最坏，平均时间复杂度均为O（nlogn）,它也是不稳定的排序 堆是具有以下性质的完全二叉树： 每个结点的值都大于或等于其左右孩子节点的值，称为大顶堆 注意：没有要求结点的左孩子的值和右孩子的值的大小关系 每个结点的值都小于或等于其左右孩子的结点，称为小顶堆 大顶堆举例说明: 我们对堆中的结点按照层次进行编号，映射到数组中就是下面这个样子： [ 50 , 45 , 40 , 20 , 25 , 35 , 30 , 10 ,15] 大顶堆特点: arr[ i ] &gt;= arr [ 2 * i + 1 ] &amp;&amp; arr[ i ] &gt;= arr [ 2 * i + 2] // i对应第几个结点，i从0开始编号 小顶堆： 小顶堆：arr [ i ] &lt; = arr [ 2 * i + 1] &amp;&amp; arr [ i ] &lt;= arr [ 2 * i + 2] 一般升序采用大顶堆，降序采用小顶堆 堆排序基本思想： 1、将待排序序列构造成一个大顶堆 2、此时，整个序列的最大值就是顶堆的根节点 3、将其与末尾元素进行交换，此时末尾就为最大值 4、然后将剩余 n - 1个元素重新构造成一个堆，这样会得到 第n 个元素的次小值，如此反复执行，便能得到一个有序序列了 例题：给定一个数组 { 4， 6 ，8 ，5 ，9 }，要求使用堆排序法，将数组升序排序 图解： step1：构造初始堆。将给定无序序列构造成一个大顶堆 （ 一般升序采用大顶堆，降序采用小顶堆） 1、假设给定无序序列结构如下： 2、此时我们从最后一个非叶子结点开始（叶子结点不用调整，最后一个非叶子结点 arr.length / 2 -1 = 5 /2 - 1 = 1 , 也就是下面的6结点），从左至右，从下至上进行调整 arr.length / 2 -1 怎么来的？ 最后一个结点对应的数组下标为 arr.lenth - 1 ; 而 父结点 为 i 的左孩子下标为：2 * i + 1 ; 右结点为 2 * i + 2; 3、找到第二个非叶子节点4，先比较左右两边，9最大，4和9交换 4、这时，交换导致了子根 【 4， 5， 6】结构混乱，继续调整， 【 4，5 ，6 】中 6 最大 ，交换 4 和 6 此时，我们就将一个无序序列构造成了一个大顶堆 Step2：将堆顶元素与末尾元素进行交换，使末尾元素最大。然后继续调整堆，再将堆顶元素与末尾元素交换，得到第二大元素。如此反复进行 堆顶元素 9 和 末尾元素 4 进行交换 交换后重新调整结构，使其满足堆定义 再将堆顶元素8与末尾元素5进行交换，得到第二大元素8 （ 9 已经搞完了，相当于把它剔除了，所以这里末尾元素是5） 后续依次反复进行调整 总结： 将无序序列构建成一个堆，根据升序降序需求选择大顶堆或小顶堆 将堆顶元素与末尾元素交换，将最大元素“沉”到数组末端 重新调整结构，使其满足堆定义，然后继续交换堆顶元素与当前末尾元素，反复执行 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class HeapSort &#123; public static void main(String[] args) &#123; // 将数组升序排列 int array[] = &#123; 4 ,6 ,8 ,5 ,9 &#125;; heapSort(array); System.out.println(Arrays.toString(array)); &#125; /** * 0 1 2 3 4 * [4 ,6 ,8 ,5 ,9 ] * 4 * / \\ * 6 8 * / \\ * 8 9 * @param array */ public static void heapSort(int[] array)&#123; /** * step1 : 构造初始堆。将给定无序序列构造成一个大顶堆 */ // 最后一个非叶子节点 的下标为 array.length / 2 - 1; // 最后第二个非叶子节点 的下标为 最后一个非叶子结点的下标 - 1 for (int i = array.length / 2 - 1; i &gt;= 0 ; i -- ) &#123; adjustBigHeap(array,i, array.length); &#125; /** * step2 : 将堆顶元素与末尾元素进行交换，使末尾元素最大。然后继续调整堆，再将堆顶元素与末尾元素交换，得到第二大元素。如此反复进行 */ int length = array.length; for (int i = 0; i &lt; array.length; i++) &#123; int root = array[0]; // 堆顶元素 // 交换位置 array[0] = array[length - 1]; array[length - 1] = root; length -- ; // 重新调整堆 adjustBigHeap(array,0,length); // 0 ——&gt; 从堆顶开始调整 &#125; &#125; /** * 将一个数组 (把数组当成二叉树的层次排序) 调整成一个大顶堆 * 【以 index 为对应的非叶子节点的树进行调整 成大顶堆】 * @param array 待调整的数组 * @param index 非叶子结点在数组中的索引 * @param length 表示对多少个元素进行调整 * @return */ public static void adjustBigHeap(int[] array,int index, int length)&#123; if (index &lt; 0)&#123; return; &#125; int temp = array[index]; // 先取出当前元素的值,存入临时变量 // 开始调整 // 【 index * 2 + 1 】 左孩子的下标 // 【 i = i * 2 + 1 】 继续往下调整，也就是左孩子的左孩子 for (int i = index * 2 + 1; i &lt; length; i = i * 2 + 1) &#123; if ( i + 1 &lt; length &amp;&amp; array [ i ] &lt; array [ i + 1 ])&#123; // 左子结点 小于 右子结点 i = i + 1; // i 指向右子节点 &#125; if ( array [ i ] &gt; temp)&#123; // 子节点 大于当前节点 // 进行调换 array[ index ] = array[i]; // 把较大得值赋给当前节点 // array[i] = temp; index = i; // index 指向 与之调换的下标 逻辑上交换，物理上不交换 // 继续循环比较 &#125;else &#123; break; &#125; &#125; // for 循环结束后，已经将 以index为顶点 的树调整为大顶堆 array[index] = temp; // 将temp放到调整后的位置 &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://c89757.gitee.io/colinstar/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"堆","slug":"堆","permalink":"http://c89757.gitee.io/colinstar/tags/%E5%A0%86/"},{"name":"排序算法","slug":"排序算法","permalink":"http://c89757.gitee.io/colinstar/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"Synchronized详解","slug":"Synchronized详解","date":"2022-01-15T14:11:43.000Z","updated":"2022-03-25T08:58:33.756Z","comments":true,"path":"2022/01/15/Synchronized详解/","link":"","permalink":"http://c89757.gitee.io/colinstar/2022/01/15/Synchronized%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"设计同步器的意义 多线程编程中，有可能会出现多个线程同时访问同一个共享，可变资源的情况下，这个资源我们称之为临界资源；这种资源可能是：对象、变量、文件等 ​ 共享：资源可以由多个线程同时访问 ​ 可变：资源可以在其生命周期内被修改 由于线程执行的过程是不可控的，所以需要采用同步机制来协同对对象可变状态的访问 如何解决线程并发安全问题？ 实际上，所有的并发模式在执行线程安全问题时，采用的方案都是序列化访问临界资源。即在同一时刻，只能有一个线程访问临界资源，也称作同步互斥访问 Java中，提供了两种方式来实现同步互斥访问：synchronized和lock同步器的本质就是加锁目的：序列化访问临界资源，即同一时刻只能有一个线程访问临界资源（同步互斥访问） synchronized内置锁是一种对象锁（锁的是对象而非引用），作用粒度是对象，可以用来实现对临界资源的同步互斥访问，是可重入的 Synchronized底层原理​ synchronized是基于JVM内置锁实现，通过内部对象Monitor(监视器锁），基于进入与退出Monitor对象实现方法与代码块同步，监视器锁的实现依赖底层操作系统的Mutex lock(互斥锁）实现，它是一个重量级锁，性能较低。当然，JVM内置锁在1.5之后版本做了重大的优化，如锁粗化(Lock Coarsening),锁消除（Lock Elimination）,轻量级锁（Lightweight Locking)、偏向锁（Biased Locking)、适应性自旋（Adaptive Spinning)等技术来减少锁操作的开销，内置锁的并发性能已经基本与Lock持平。Synchronized关键字被编译成字节码后会被翻译成 monitorenter 和monitorexit 两条指令分别在同步块逻辑代码的起始位置与结束位置 每个对象都有一个自己的monitor(监视器锁)，加锁过程如下 Monitor监视器锁​ ​ 任何一个对象都有一个Monitor与之关联，当且一个Monitor被持有后，它将处于锁定状态。Synchronized在JVM里的实现都是 基于进入和退出Monitor对象来实现方法同步和代码块同步，虽然具体实现细节不一样，但是都可以通过成对的MonitorEnter和MonitorExit指令来实现。 monitorenter：每个对象都是一个监视器锁（Monitor）。当monitor被占用时就会处于锁定状态，线程执行monitorenter指令时尝试获取monitor的所有权，过程如下： 如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者 如果该线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1 如果其他线程占有该monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取monitor的所有权 monitorexit：执行monitorexit的线程必须是object ref对应的monitor的所有者。指令执行时，monitor的进入数减1，如果减1后进入数为0，那线程退出monitor,步再是这个monitor的所有者，其他被这个monitor阻塞的线程可以尝试去获取这个monitor的所有权 ​ Synchronized的语义底层是通过一个monitor的对象来完成，其实wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。 123public synchronized void method()&#123; System.out.println(&quot;hello world&quot;);&#125; 经过javap解析后如下 ​ 方法的同步并没有通过指令 monitorenter 和 monitorexit 来完成（理论上其实也可以通过这两条指令来实现），不过相对于普通方法，其常量池中多了ACC_SYNCHRONIZED 标示符。JVM就是根据该标示符来实现方法的同步的：当方法调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个 monitor对象。 什么是monitor?​ ​ 可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。与一切皆对象一样，所有的java对象是天生的Monitor，每一个Java对象都有称为Monitor的潜质，因为在java的设计中，每一个Java对象自生成就带了把看不见的锁，它叫做内置锁或者Monitor锁；也就是通常说的Synchronized的对象锁，MarkWord锁标识位为10，其中指针指向的是Monitor对象的起始位置；在Java虚拟机（HotSpot）中，Monitor是由ObjectMonitor实现的，其主要数据结构如下（位于HotSpot虚拟机源码ObjectMonitor.hpp文件，C++实现的） ​ ​ 我们知道synchronized加锁加在对象上，对象是如何记录锁状态的呢？答案是锁状态是被记录在每个对象的对象头（Mark Word）中，下面我们一起认识一下对象的内存布局 对象的内存布局​ ​ HotSpot虚拟机中，对象在内存中存储的布局分为三块区域：对象头（Header)、示例数据（Instance Data）和对齐填充（Padding） 对象头：保存对象的Hash码，GC年龄，对象锁，锁状态标致，偏向锁（线程）ID，偏向时间等，如果是数组对象，还会保存数组的长度。Java对象头一般占有2个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit，在64位虚拟机中，1个机器码是8个字节，也就是64bit)；但是如果对象是数组类型，则需要3个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确定数组的大小，所以用一块来记录数组的长度 实例数据：存放类的属性数据信息，包括父类的属性信息 对齐填充：由于虚拟机要求对象起始地址必须是8字节的整数倍，填充数据不是必须存在的，仅仅是为了字节对齐 ​ HotSpot虚拟机的对象头包括两部分信息，第一部分是”Mark Word”，用于存储对象自身的运行时数据，如哈希码（HashCode），GC分代年龄、锁状态标志，线程持有的锁，偏向线程ID，偏向时间戳等等，它是实现轻量级锁和偏向锁的关键。这部分数据的长度在32位和64位的虚拟机（暂不考虑开启压缩指针的场景）中分别位32个和64个Bits，官方称它为“Mark Word”。对象需要存储的运行时数据很多，其实已经超出了32、64位Bitmap结构所能记录的限度，但是对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。例如：在32位的HotSpot虚拟机 中对象未被锁定的状态下，Mark Word的32个Bits空间中的25Bits用于存储对象哈希码（HashCode），4Bits用于存储对象分代年龄，2Bits用于存储锁标志位，1Bit固定为0，在其他状态（轻量级锁定、重量级锁定、GC标记、可偏向）下对象的存储内容如下表所示 32位虚拟机 64位虚拟机 现在的虚拟机基本上是64位的，而64位的对象头有点浪费空间，JVM默认会开启指针压缩，所以基本上也是按32位的形式记录对象头的。 手动设置: -XX:+UseCompressedOops 哪些信息会被压缩？ 1.对象的全局静态变量(即类属性) 2.对象头信息：64位平台下，原生对象头大小为16字节，压缩后为12字节 3.对象的引用类型：64位平台下，引用类型本身大小为8字节，压缩后为4字节 4.对象数组类型：64位平台下，数组类型本身大小为24字节，压缩后16字节 对象头分析工具 OpenJDK开源工具包，JOL，maven坐标如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt;&lt;/dependency&gt; 案例： 12345public static void main(String[] args) &#123; Object o = new Object(); // 打印markword System.out.println(ClassLayout.parseInstance(o).toPrintable());&#125; 打印出来的对象内存信息如下： 12345678java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1)//Mark Word 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 第一行为Mark Word 00000001 00000000 00000000 00000000 对象此时是无锁状态，前25位表示hashcode值，为什么hashcode是0？ 因为这个hashcode是jvm内置函数，类似于懒加载，此时还没有计算 此时将代码修改为如下： 1234567public static void main(String[] args) &#123; Object o = new Object(); System.out.println(ClassLayout.parseInstance(o).toPrintable()); synchronized (o)&#123; System.out.println(ClassLayout.parseInstance(o).toPrintable()); &#125;&#125; 打印的Mark Word为 00011000 11110111 00000010 00000010 即 00000010 00000010 11110111 00011000 发现对象头从无锁——&gt;轻量级锁 为什么不是偏向锁？ ​ 因为JVM会延迟去启动偏向锁，JVM启动时依赖大量的hashMap class对象等，这些对象里面也存在大量的同步块，JVM启动时内部也会去启动十几个线程，这些线程内部也会存在竞争，JVM为了避免造成 偏向锁 到 轻量级锁 到重量级锁 这种锁升级过程，减少锁升级的开销，所以把偏向锁推迟启动了 将代码睡眠几秒钟 12345678public static void main(String[] args) throws InterruptedException &#123; TimeUnit.SECONDS.sleep(10); Object o = new Object(); System.out.println(ClassLayout.parseInstance(o).toPrintable()); synchronized (o)&#123; System.out.println(ClassLayout.parseInstance(o).toPrintable()); &#125;&#125; 12345678910111213141516171819java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 05 00 00 00 (00000101 00000000 00000000 00000000) (5) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total java.lang.Object object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) a8 f7 06 03 (10101000 11110111 00000110 00000011) (50788264) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 第一次打印的：00000101 00000000 00000000 00000000 就已经是偏向锁状态了，但是偏向锁的前23bit位会记录线程ID，此处并没有，这种 称之为匿名偏向，可偏向状态 如果一直处于偏向状态，无法重偏向的话，那么MarkWord会一直记录最后一个偏向线程的状态 锁的膨胀升级过程​ ​ 锁的状态总共有四种，无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级到重量级锁，但是锁的升级是单向的，也就是只能从低到高升级，不会出现锁的降级。从JDK1.6中默认是开启偏向锁和轻量级的，可以通过-XX:-UseBiasedLocking来禁用偏向锁 偏向锁：​ 偏向锁是Java6之后加入的新锁，它是一种针对加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁（会涉及到一些CAS操作，耗时）的代价而引入偏向锁。偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提高了程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。 默认开启偏向锁 开启偏向锁：-XX:+UseBiasedLocking -XX:BiasedLockingStartupDelay=0 关闭偏向锁：-XX:-UseBiasedLocking 轻量级锁：​ 倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)，此时，Mark Word的结构也变为轻量级锁的结构。轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步周期内都不存在竞争”，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，就会导致轻量级锁膨胀为重量级锁 自旋锁： 轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，因此自旋锁会假设在不久将来，当前的线程就可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环（这也是称为自旋的原因），一般不会太久，可能是50个循环或100循环，在经过若干次循环后，如果得到锁，就顺利进入临界区。如果还不能获得锁，那就会将线程在操作系统层面挂起，这就是自旋锁的优化方式，这种方式确实也是可以提升效率的。后没办法也就只能升级为重量级锁 锁消除: ​ 消除锁是虚拟机另外一种锁的优化，这种优化更彻底，Java虚拟机在JIT编译时，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间。锁消除的依据是逃逸分析的数据支持 锁消除，前提是Java必须运行在server模式，（server模式会比client模式作更多的优化），同时必须开启逃逸分析 -XX:+DoEscapeAnalysis 开启逃逸分析 -XX:+EliminateLocks 表示开启锁消除 使用逃逸分析，编译器可以对代码做如下优化： 同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步 将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配 分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在CPU寄存器中","categories":[{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/categories/JUC/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/tags/JUC/"},{"name":"多线程","slug":"多线程","permalink":"http://c89757.gitee.io/colinstar/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Netty","slug":"Netty","date":"2022-01-06T12:58:22.000Z","updated":"2022-03-25T08:55:54.425Z","comments":true,"path":"2022/01/06/Netty/","link":"","permalink":"http://c89757.gitee.io/colinstar/2022/01/06/Netty/","excerpt":"","text":"BIO&amp;NIO&amp;AIOBIO ​ blocking I/O , 即阻塞IO，同步阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时，服务端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，可以通过线程池机制改善（实现多个客户连接服务器） 先看单线程的版本 12345678910111213141516public static void main(String[] args) throws IOException &#123; final ServerSocket serverSocket = new ServerSocket(9000); while (true)&#123; log.info(&quot;等待连接....&quot;); final Socket socket = serverSocket.accept(); log.info(&quot;建立连接&quot;); InputStream inputStream = socket.getInputStream(); byte[] bytes = new byte[1024]; int read = inputStream.read(bytes); if (read != -1)&#123; log.info(&quot;收到消息：&#123;&#125;&quot;,new String(bytes,0,read)); &#125; socket.getOutputStream().write(&quot;已成功接收到消息&quot;.getBytes()); socket.getOutputStream().flush(); &#125; &#125; Debug启动程序，在serverSocket.accept()处打上断点，同时再下一行处也打上断点，然后我们点击idea调试的Resume Program按钮，让程序直接走完；我们会发现断点没有到达下一行，程序也没有停止，而是阻塞在了accept()里。 我们试着用telnet工具去连接程序 按下回车连接的同时，我们也会发现程序的断点跑到了下一行 我们再在 int read = inputStream.read(bytes);这一行及其下一行也打上断点； 程序来到inputStream.read(bytes)这一行，我们再次选择放掉这一个断点，发现此处程序也并没有来到下一行，也是在此处进行了阻塞 我们用telnet工具给服务端发送消息 回到程序，发现程序执行到了下一行 接下来我们重新开始，重新启动服务端，开启一个telnet（客户端1）去连接，但是不发送消息，让程序阻塞在int read = inputStream.read(bytes)这一行；与此同时，我们再另外开启一个telnet客户端（客户端2）去进行连接，然后发送消息给服务端 但是我们发现，控制台并没有任何消息打印； 我们此时在用客户端1去发送消息 发现客户端打印消息，但是打印hello2之前，输出了”建立连接“；说明此时我们其实客户端2并没有真正的连接上，而是阻塞在了serverSocket.accept()处 12final Socket socket = serverSocket.accept();log.info(&quot;建立连接&quot;); 在同一时刻，服务端只能响应一个客户端 解决方案我们可以在将代码改成多线程版本 12345678910111213141516171819202122232425262728293031323334353637383940414243@Slf4jpublic class TestBIO extends Thread&#123; private Socket socket; public TestBIO(Socket socket) &#123; this.socket = socket; &#125; public static void main(String[] args) throws IOException &#123; final ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5000, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); log.info(&quot;等待连接....&quot;); final ServerSocket serverSocket = new ServerSocket(9000); while (true)&#123; Socket socket = serverSocket.accept(); log.info(&quot;建立连接&quot;); threadPoolExecutor.execute(new TestBIO(socket)); if (false)&#123; break;&#125; &#125; &#125; public static void handler(Socket socket) throws IOException &#123; InputStream inputStream = socket.getInputStream(); byte[] bytes = new byte[1024]; int read = inputStream.read(bytes); if (read != -1)&#123; log.info(&quot;收到消息：&#123;&#125;&quot;,new String(bytes,0,read)); &#125; socket.getOutputStream().write(&quot;已成功接收到消息&quot;.getBytes()); socket.getOutputStream().flush(); &#125; @Override public void run() &#123; try &#123; handler(this.socket); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 我们再实验以上步骤 先开启telnet客户端1去连接阻塞，但是不发送消息 再开启telnet客户端2去连接，并发送消息 结果此次控制台能正确接收到消息 存在的问题​ 如果开辟大量线程，比较消耗资源，且如果我们用了线程池，如果我们线程池数量是500，某一瞬间并发量有1w，那后面的请求就只能阻塞等待。又或者500线程池，其中400个线程只是和你建立连接，并不立马发送消息给服务端，那这个线程会一直被这个连接给占用，其他人无法获取; 又或者用完线程给别人用时，线程的切换也是比较消耗资源的 IO代码里read操作是阻塞操作，如果连接不做数据读写会导致线程阻塞，浪费资源 如果线程很多，会导致服务器线程太大，压力太大 应用场景：BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高 NIO​ Non Blocking IO,或者读为New IO,同步非阻塞，服务器实现模式为一个线程可以处理多个请求（连接），客户端发送的连接请求都会注册到多路复用器selector上，多路复用器轮询到连接有IO请求就进行处理，JDK1.4开始引入 123456789101112131415161718192021222324252627282930313233343536373839@Slf4jpublic class TestNIO &#123; private static List&lt;SocketChannel&gt; channelList = new ArrayList&lt;&gt;(); public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(9000)); // 设置ServerSocketChannel为非阻塞 serverSocketChannel.configureBlocking(false); while (true) &#123; // 非阻塞模式accept方法不会阻塞，否则会阻塞 // NIO的非阻塞是由操作系统内部实现的，底层调用了linux内核的accept函数 SocketChannel socketChannel = serverSocketChannel.accept(); if (socketChannel != null) &#123; // 如果有客户端进行连接 log.info(&quot;连接成功&quot;); // 设置SocketChannel为非阻塞 socketChannel.configureBlocking(false); // 保存客户端连接在list中 channelList.add(socketChannel); &#125; // 遍历连接进行数据读取 Iterator&lt;SocketChannel&gt; iterator = channelList.iterator(); while (iterator.hasNext()) &#123; SocketChannel next = iterator.next(); ByteBuffer byteBuffer = ByteBuffer.allocate(128); // 非阻塞模式read方法不会阻塞 int len = next.read(byteBuffer); if (len &gt; 0) &#123; log.info(&quot;接收到消息: &#123;&#125;&quot;, new String(byteBuffer.array())); &#125; else if (len == -1) &#123; // 如果客户端断开，把socket从集合中删调 iterator.remove(); log.info(&quot;与客户端断开连接&quot;); &#125; &#125; &#125; &#125;&#125; 我们先后开启两个telnet客户端去连接服务端，发送消息，服务端都能接收到; 会一直循环去判断是否有新的连接请求，是否有连接发送消息 上述代码存在的问题： 如果连接数太多的话，会有大量的无效遍历 比如如果我现在有10万个连接，但是经常给服务端发消息的就那个几百个，但是每次都要去遍历所有的连接 我们可以将那些有数据交互的连接，存储在另外一个数据结构中，每次遍历只需要遍历那些有数据交互的连接 NIO引入多路复用器代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Slf4jpublic class NioSelectorServer &#123; public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(9000)); // 设置ServerSocketChannel为非阻塞 serverSocketChannel.configureBlocking(false); // 打开selector处理Channel，即创建epoll Selector selector = Selector.open(); // 把ServerSocketChannel注册到selector上，并且selector监听客户端accept连接事件 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; // 阻塞等待需要处理的事件发生 selector.select(); // 获取selector中注册的全部事件的SelectionKey实例 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); // 遍历SelectionKey对事件进行处理 while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); // 如果是OP_ACCEPT事件，则进行连接获取和事件注册 if (key.isAcceptable())&#123; ServerSocketChannel channel =(ServerSocketChannel) key.channel(); final SocketChannel socketChannel = channel.accept(); socketChannel.configureBlocking(false); // 这里只注册了读事件，如果需要给客户端发送数据可以注册写事件 socketChannel.register(selector,SelectionKey.OP_READ); log.info(&quot;客户端连接成功&quot;); &#125;else if(key.isReadable())&#123; // 如果是OP_READ事件，则进行读取和打印 SocketChannel channel =(SocketChannel)key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); int len = channel.read(byteBuffer); if (len &gt;0 )&#123; log.info(&quot;接收到消息:&#123;&#125;&quot;,new String(byteBuffer.array())); &#125;else if(len == -1)&#123; log.info(&quot;客户端断开连接&quot;); channel.close(); &#125; &#125; // 从事件集合里删除本次处理的key,防止下次select重复处理 iterator.remove(); &#125; &#125; &#125;&#125; NIO有三大核心组件：Channel(通道)，Buffer(缓冲区)，Selector(多路复用器) 1、channel类似于流，每个channel对应一个buffer缓冲区，buffer底层就是个数组 2、channel会注册到selector上，由selector根据channel独写事件的发生将其交由某个空闲的线程处理 3、NIO的Buffer和channe都是既可以读也可以写 先看一幅图 ​ 我们代码最开始处，创建了一个ServerSocketChannel，并绑定9000端口,并将ServerSocketChannel注册到selector上，并且selector监听客户端accept连接事件，注册上后会返回一个key,通过这个selectionKey可以找到与之绑定的ServerSocketChannel; ​ 我们在selector.select()处及其下一行打上断点，启动项目。 ​ 放掉断点让其走完，发现程序阻塞在了这一行； ​ 同样的，打开cmd，用telnet连接 1telnet localhost 9000 ​ ​ 连接上后，发现程序走到了下一行 ​ 继续往下走一行，获取到所有的selectionKey; 因为此时我们只有一个客户端进行连接，所以此处size是1 ​ 很显然我们此处是OP_ACCEPT事件 ​ 通过selectionKey可以拿到与之绑定的ServerSocketChannel，并让其与客户端建立连接,并把客户端对应的socketChannel也注册到selector上，并让其监听读事件（读是相当于服务端来的，也就是监听客户端发送过来的消息） ​ 我们一步一步调试，让程序走完，因为是死循环，在select处又会进行阻塞，因为此时既没有新的客户端连接进来，刚刚连接上的客户端也没有发送消息。 ​ 我们用telnet再给服务端发送一条消息 ​ 此时，程序停止了阻塞，走到了下一行 ​ 一步一步调试，很显然这次我们是OP_READ事件，通过key拿到与客户端对应的SocketChannel。也就是下图标识出来的部分，用它来读取客户端的数据 ​ 我们现在再另外开启一个telnet客户端，连接服务端 ​ 我们可以看到，现在有两个客户端，但是拿到的selectionKey只有一个，只针对那些发生的事件进行处理 ​ NIO底层在JDK1.4版本是用linux的内核函数select()或poll()来实现，跟上面最开始的代码类似，selector每次都会轮询所有的socketChannel看下哪个channel有读写事件，有的话就处理，没有就继续遍历，JDK1.5引入了epoll基于事件响应机制来优化NIO 几个核心APISelector.open();1Selector selector = Selector.open(); provider()方法里最终调用了下面的create()方法，发现其new 了一个WindowsSelectorProvider()。因为我们日常使用的是windows的jdk 123public static Selector open() throws IOException &#123; return SelectorProvider.provider().openSelector();&#125; 12345678public class DefaultSelectorProvider &#123; private DefaultSelectorProvider() &#123; &#125; public static SelectorProvider create() &#123; return new WindowsSelectorProvider(); &#125;&#125; 下载openJdk8u的源码，搜索DefaultSelectorProvider这个类，发现有三个，分别对应unix系统，mac系统，windows系统。我们接下来看unix系统对应的源码 unix系统create()方法的源码如下，发现和windows的有区别，如果是linux系统，会返回EPollSelectorProvider这个类 123456789public static SelectorProvider create() &#123; String osname = AccessController .doPrivileged(new GetPropertyAction(&quot;os.name&quot;)); if (osname.equals(&quot;SunOS&quot;)) return createProvider(&quot;sun.nio.ch.DevPollSelectorProvider&quot;); if (osname.equals(&quot;Linux&quot;)) return createProvider(&quot;sun.nio.ch.EPollSelectorProvider&quot;); return new sun.nio.ch.PollSelectorProvider();&#125; open()方法里会调用openSelector()这个方法，EPollSelectorProvider里的实现如下，直接new 了一个EPollSelectorImpl 1234567891011public class EPollSelectorProvider extends SelectorProviderImpl&#123; public AbstractSelector openSelector() throws IOException &#123; return new EPollSelectorImpl(this); &#125; public Channel inheritedChannel() throws IOException &#123; return InheritedChannel.getChannel(); &#125;&#125; 接着我们去看看EpollSelectorImpl这个类的构造函数，初始化的时候， new EPollArrayWrapper()创建了一个EPollArrayWrapper对象 123456789EPollSelectorImpl(SelectorProvider sp) throws IOException &#123; super(sp); long pipeFds = IOUtil.makePipe(false); fd0 = (int) (pipeFds &gt;&gt;&gt; 32); fd1 = (int) pipeFds; pollWrapper = new EPollArrayWrapper(); pollWrapper.initInterrupt(fd0, fd1); fdToKey = new HashMap&lt;&gt;();&#125; 紧接着我们看到EPollArrayWrapper的构造函数，里面调用了一个epollCreate（）方法 12345678910111213EPollArrayWrapper() throws IOException &#123; // creates the epoll file descriptor epfd = epollCreate(); // the epoll_event array passed to epoll_wait int allocationSize = NUM_EPOLLEVENTS * SIZE_EPOLLEVENT; pollArray = new AllocatedNativeObject(allocationSize, true); pollArrayAddress = pollArray.address(); // eventHigh needed when using file descriptors &gt; 64k if (OPEN_MAX &gt; MAX_UPDATE_ARRAY_SIZE) eventsHigh = new HashMap&lt;&gt;();&#125; ​ epollCreate是一个本地方法 （java的native方法是通过JNI，即java native interface来实现的，可以通过它来实现java与其他语言之间的交互） 1private native int epollCreate(); EPollArrayWrapper.c里找到这个epollCreate方法, epoll_create是linux的一个系统函数 12345678910111213JNIEXPORT jint JNICALLJava_sun_nio_ch_EPollArrayWrapper_epollCreate(JNIEnv *env, jobject this)&#123; /* * epoll_create expects a size as a hint to the kernel about how to * dimension internal structures. We can&#x27;t predict the size in advance. */ int epfd = epoll_create(256); if (epfd &lt; 0) &#123; JNU_ThrowIOExceptionWithLastError(env, &quot;epoll_create failed&quot;); &#125; return epfd;&#125; 我们在linux系统上执行 man epoll_create命令，查看这个函数的文档 -打开一个文件描述符，相当于创建了一个epoll对象，返回文件描述符的索引 int epfd = epoll_create(256) serverSocketChannel.register(…) 1serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); java.nio.channels.SelectableChannel#register(java.nio.channels.Selector, int) 12345public final SelectionKey register(Selector sel, int ops) throws ClosedChannelException&#123; return register(sel, ops, null);&#125; java.nio.channels.spi.AbstractSelectableChannel#register 这个方法里面最终又调用了一个register方法，我们再点进去 12345678910111213141516public final SelectionKey register(Selector sel, int ops, Object att) throws ClosedChannelException&#123; synchronized (regLock) &#123; ..... synchronized (keyLock) &#123; if (!isOpen()) throw new ClosedChannelException(); k = ((AbstractSelector)sel).register(this, ops, att); // 主要看这个register方法 addKey(k); &#125; &#125; ..... &#125;&#125; sun.nio.ch.SelectorImpl#register 这个里面又调用了一个implRegister（）方法，我们点进去是个抽象方法 protected abstract void implRegister(SelectionKeyImpl var1); 查看他的实现类，来到了WindowsSelectorImpl，这是windows系统的实现，我们去查看linux的implRegister的实现方法 123456789protected final SelectionKey register(AbstractSelectableChannel var1, int var2, Object var3) &#123; .... synchronized(this.publicKeys) &#123; this.implRegister(var4); &#125; .... &#125;&#125; sun.nio.ch.EPollSelectorImpl#implRegister pollWrapper.add(fd); fd是文件描述符，会根据这个索引找到这个文件（linux一切皆文件），在此处就是linux系统能够根据pd这个文件描述符找到这个创建好的serverSocketChannel； 这个pollWrapper就是上面Selector.open()里创建的pollWrapper 123456789protected void implRegister(SelectionKeyImpl ski) &#123; if (closed) throw new ClosedSelectorException(); SelChImpl ch = ski.channel; int fd = Integer.valueOf(ch.getFDVal()); fdToKey.put(fd, ski); pollWrapper.add(fd); keys.add(ski);&#125; selector.select(); 123Selector selector = Selector.open(); ....selector.select(); select是一个抽象方法 1public abstract int select() throws IOException; 点进实现类 sun.nio.ch.SelectorImpl#select() 123public int select() throws IOException &#123; return this.select(0L);&#125; lockAndDoSelect 1234567public int select(long var1) throws IOException &#123; if (var1 &lt; 0L) &#123; throw new IllegalArgumentException(&quot;Negative timeout&quot;); &#125; else &#123; return this.lockAndDoSelect(var1 == 0L ? -1L : var1); &#125;&#125; sun.nio.ch.SelectorImpl#lockAndDoSelect 123456789101112private int lockAndDoSelect(long var1) throws IOException &#123; ...... synchronized(this.publicKeys) &#123; synchronized(this.publicSelectedKeys) &#123; var10000 = this.doSelect(var1); &#125; &#125; ...... &#125; &#125;&#125; doSelect是一个抽象方法，点进实现类来到了WindowsSelectorImpl。同样的，我们需要看linux的实现EPollSelectorImpl 1protected abstract int doSelect(long var1) throws IOException; sun.nio.ch.EPollSelectorImpl#doSelect 1234567protected int doSelect(long timeout) throws IOException &#123; ....... pollWrapper.poll(timeout); ....... &#125; return numKeysUpdated;&#125; sun.nio.ch.EPollArrayWrapper#poll 123456789101112int poll(long timeout) throws IOException &#123; updateRegistrations(); updated = epollWait(pollArrayAddress, NUM_EPOLLEVENTS, timeout, epfd); for (int i=0; i&lt;updated; i++) &#123; if (getDescriptor(i) == incomingInterruptFD) &#123; interruptedIndex = i; interrupted = true; break; &#125; &#125; return updated;&#125; 先看updateRegistrations（）方法 updateRegistrations(); 此方法里又调用了一个epollCtl(epfd, opcode, fd, events) ，点进去，这是一个本地方法 private native void epollCtl(int epfd, int opcode, int fd, int events); 内部调用的就是linux函数epoll_ctl 123456789101112131415161718192021222324252627282930private void updateRegistrations() &#123; synchronized (updateLock) &#123; int j = 0; while (j &lt; updateCount) &#123; int fd = updateDescriptors[j]; short events = getUpdateEvents(fd); boolean isRegistered = registered.get(fd); int opcode = 0; if (events != KILLED) &#123; if (isRegistered) &#123; opcode = (events != 0) ? EPOLL_CTL_MOD : EPOLL_CTL_DEL; &#125; else &#123; opcode = (events != 0) ? EPOLL_CTL_ADD : 0; &#125; if (opcode != 0) &#123; epollCtl(epfd, opcode, fd, events); if (opcode == EPOLL_CTL_ADD) &#123; registered.set(fd); &#125; else if (opcode == EPOLL_CTL_DEL) &#123; registered.clear(fd); &#125; &#125; &#125; j++; &#125; updateCount = 0; &#125; &#125; 在linux系统上执行命令 1man epoll_ctl 查看此函数 epollCtl(epfd, opcode, fd, events); epfd epoll实例对应的文件描述符 fd socketChannel对应的文件描述符events 事件 参数opcode又以下几个值： 123EPOLL_CTL_ADD // 注册新的SocketChannel到epoll实例中，并关联事件eventEPOLL_CTL_DEL // 修改已经注册的SocketChannel的监听事件EPOLL_CTL_MOD // 从epoll中移除SocketChannel，并且忽略掉绑定的event epollCtl这个方法把SocketChannel和epoll关联起来 2.updated = epollWait(pollArrayAddress, NUM_EPOLLEVENTS, timeout, epfd); 再回到poll方法里，程序继续往下走，接着看epollWait这个方法，点进去也是一个本地方法，也是调用的操作系统内核函数 epoll_wait epoll_wait, epoll_pwait - wait for an I/O event on an epoll file descriptor epoll_wait的时候，会去查看sector里面的rdlist就绪列表里是否有数据，有数据就跳出阻塞，没有就阻塞住 利用操作系统回调函数，客户端有响应，把事件放进rdlist AIO（NIO 2.0） ​ 异步非阻塞，由操作系统完成后回调通知服务端程序启动线程去处理，一般适用于连接数较多并且连接时间较长的应用 代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940public class TestAIO &#123; public static void main(String[] args) throws IOException &#123; AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(9000)); serverSocketChannel.accept(null, new CompletionHandler&lt;AsynchronousSocketChannel, Object&gt;() &#123; @Override public void completed(AsynchronousSocketChannel socketChannel, Object attachment) &#123; try &#123; System.out.println(&quot;2----&quot; + Thread.currentThread().getName()); // 再此接收客户端连接,如果不写这行代码后面的客户端连接不上服务端 serverSocketChannel.accept(attachment, this); System.out.println(socketChannel.getRemoteAddress()); ByteBuffer buffer = ByteBuffer.allocate(1024); socketChannel.read(buffer, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; System.out.println(&quot;3---&quot; + Thread.currentThread().getName()); buffer.flip(); System.out.println(new String(buffer.array(), 0, result)); socketChannel.write(ByteBuffer.wrap(&quot;HelloClient&quot;.getBytes())); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; exc.printStackTrace(); &#125; &#125;); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; @Override public void failed(Throwable exc, Object attachment) &#123; exc.printStackTrace(); &#125; &#125;); System.out.println(&quot;1---&quot;+Thread.currentThread().getName()); System.in.read(); &#125;&#125; NettyRector响应式编程模型 所谓响应式，类似于GUI编程中，给一个Button绑定一个或多个事件，对于点击事件，点击后会触发对应方法 基础的Reactor设计，单线程版本; 用一个Reactor去处理客户端的连接，以及读写 存在的问题：类比上面的NioSelectorServer类代码。selector.select()处会阻塞，同一时间如果有大量的读写事件发生，那么循环处理的时候，会耗费大量时间，而此时新进来的连接就会阻塞在selector.select()处。 我们可以引入线程池，将读写工作交给其他线程去处理 存在的问题：还是类比上面的NioSelectorServer类的代码；我们现在while (iterator.hasNext()) {….}循环去处理的时候，引入线程池，将读写事件交给线程池去处理；这样分发完后，主线程能很快的回到selector.select()处，阻塞监听新的事件。但是如果一时间的事件很多，那么分发都需要花费大量的时间，同样新进来的事件也得不到处理 引入两个Reactor，一个mainReactor专门用来处理连接事件。subReactor用来处理独写事件，并且把这些读写事件分发给线程池去完成 Demo NettyServer.java 1234567891011121314151617181920212223242526272829303132333435363738public class NettyServer &#123; public static void main(String[] args) &#123; // 创建两个线程组boss和worker;含有的子线程NioEventLoop的个数默认为cpu核数的两倍 // boss组只是处理连接请求，真正的和客户端业务处理，会交给worker NioEventLoopGroup bossGroup = new NioEventLoopGroup(1); // 相当于主Reactor NioEventLoopGroup workerGroup = new NioEventLoopGroup(8); // 相当于从Reactor try&#123; // 创建服务端的启动对象 ServerBootstrap serverBootstrap = new ServerBootstrap(); // 使用链式编程来配置参数 serverBootstrap.group(bossGroup,workerGroup) // 设置两个线程组 // 使用NioServerSocketChannel作为服务器的通道实现 .channel(NioServerSocketChannel.class) // 初始化服务器连接队列大小，服务端处理客户端连接请求是顺序处理的，所以同一时间处理一个客户端 // 多个客户端同时来连接的时候，服务端将不能处理的客户端连接请求放在队列中等待处理 .option(ChannelOption.SO_BACKLOG,1024) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new NettyServerHandler()); &#125; &#125;); log.info(&quot;------netty server start...------&quot;); // 绑定一个端口并且同步，生成了一个ChannelFuture异步对象，通过isDone()等方法可以判断异步事件的执行情况 // 启动服务器(并绑定端口),bind是异步操作，sync方法是等待异步操作执行完毕 ChannelFuture sync = serverBootstrap.bind(9000).sync(); // 等待服务端监听端口关闭，closeFuture是异步操作 // 通过sync方法同步等待通道关闭处理完毕，这里会阻塞等待通道关闭，内部调用的是object.wait()方法 sync.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; NettyServerHandler.Java 123456789101112131415161718192021222324252627282930313233public class NettyServerHandler extends ChannelInboundHandlerAdapter &#123; /** * 读取客户端发送的消息 */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf in = (ByteBuf) msg; System.out.println(in.toString(CharsetUtil.UTF_8)); &#125; /** * 数据读取完毕处理方法 */ @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; String msg = &quot;[来自服务端]:congratulations~连接成功&quot;; final ByteBuf byteBuf = Unpooled.copiedBuffer(msg.getBytes(CharsetUtil.UTF_8)); ctx.writeAndFlush(byteBuf); &#125; /** * 异常处理 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; // (4) // Close the connection when an exception is raised. cause.printStackTrace(); ctx.close(); &#125;&#125; NettyClient.java 123456789101112131415161718192021222324252627public class NettyClient &#123; public static void main(String[] args) throws InterruptedException &#123; // 客户端需要一个事件循环组 NioEventLoopGroup group = new NioEventLoopGroup(); try &#123; // 创建客户端启动对象 Bootstrap b = new Bootstrap(); // (1) b.group(group); // (2) b.channel(NioSocketChannel.class); // (3) b.option(ChannelOption.SO_KEEPALIVE, true); // (4) b.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new NettyClientChannel()); &#125; &#125;); System.out.println(&quot;netty client start....&quot;); // Start the client. ChannelFuture f = b.connect(&quot;127.0.0.1&quot;, 9000).sync(); // (5) // Wait until the connection is closed. f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully(); &#125; &#125;&#125; NettyClientChannel.java 1234567891011121314151617181920212223242526272829public class NettyClientChannel extends ChannelInboundHandlerAdapter &#123; /** * 当通道有读取事件时，也就是服务端发送消息 */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf m = (ByteBuf) msg; try &#123; System.out.println(m.toString(CharsetUtil.UTF_8)); &#125; finally &#123; m.release(); &#125; &#125; /** * 当客户端连接服务器完成就会触发该方法 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; final ChannelFuture f = ctx.writeAndFlush(Unpooled.copiedBuffer((&quot;[来自客户端]:hello server&quot; ).getBytes(CharsetUtil.UTF_8))); // (3) &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; Netty模块组件 下面部分摘抄自官方文档https://netty.io/wiki/user-guide-for-4.x.html#wiki-h3-9 1、NioEventLoopGroup是一个处理 I/O 操作的多线程事件循环，Netty提供各种NioEventLoopGroup为不同类型的传输提供了各种实现；实现一个服务器端应用程序，因此NioEventLoopGroup将使用两个。第一个，通常称为“boss”，接受传入连接。第二个，通常称为“worker”，一旦boss接受连接并将接受的连接注册到worker;有多少线程使用和它们是如何映射到创建渠道取决于EventLoopGroup通过构造函数实现,甚至可能是可配置的 2、ServerBootstrap is a helper class that sets up a server；ServerBootstrap是一个设置服务器的辅助类。 Bootstrap与ServerBootstrap类似，除了它是非服务端通道，如客户端或无连接通道。 如果您只指定一个EventLoopGroup，它将同时用作boss组和worker组。但是，boss组不用于客户端。 NioSocketChannel被用来创建客户端Channel，而不是NioServerSocketChannel； 注意我们不像ServerBootstrap处，使用childOption(),因为客户端SocketChannel没有父级 我们应该调用connect()方法而不是bind()方法。 3、 Netty 抽象出两组线程池BossGroup和WorkerGroup，BossGroup专门负责接收客户端的连接, WorkerGroup专 门负责网络的 4 、BossGroup和WorkerGroup类型都是NioEventLoopGroup 5、NioEventLoopGroup 相当于一个事件循环线程组, 这个组中含有多个事件循环线程 ， 每一个事件循环线程是 NioEventLoop 6、每个NioEventLoop都有一个selector , 用于监听注册在其上的socketChannel的网络通讯 7、每个Boss NioEventLoop线程内部循环执行的步骤有 3 步 ​ 处理accept事件，与client建立连接，生成NioSocketChannle ​ 将NioSocketChannel注册到某个worker NIOEventLoop上的selector ​ 处理任务队列的任务，即runAllTasks 8、每个worker NIOEventLoop线程循环执行的步骤 ​ 轮询注册到自己selector上的所有NioSocketChannel 的read, write事件 ​ 处理 I/O 事件， 即read , write 事件， 在对应 NioSocketChannel 处理业务 ​ runAllTasks处理任务队列TaskQueue的任务 ，一些耗时的业务处理一般可以放入TaskQueue中慢慢处 理，这样不影响数据在 pipeline 中的流动处理 9、每个worker NIOEventLoop处理NioSocketChannel业务时，会使用 pipeline (管道)，管道中维护了很多 handler 处理器用来处理 channel 中的数据 Bootstrap、ServerBootstrap 一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组 件，Netty 中 Bootstrap 类是客户端程序的启动引导类，ServerBootstrap 是服务端启动引导类。 Future、ChannelFuture 在Netty中所有的IO操作都是异步的，不能立刻得知消息是否被正确处理 但是可以等他执行完成或者直接注册一个监听，具体的实现就是通过Future和CahnnelFutures,他们可以注 册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件 Channel Netty网络通信的组件，能够用于执行网络I/O操作。Channel为用户提供 1、当前网络连接的通道的状态（例如是否打开？是否已连接） 2、网络连接的配置参数（例如接收缓冲区大小） 3、提供异步的网络I/O操作（如建立连接，读写，绑定端口），异步调用意味着任何I/O调用都将立即返回，并且不保证在调用结束时所请求的I/O操作已完成 4、调用立即返回一个ChannelFuture实例，通过注册监听器到ChannelFuture上，可以I/O操作成功、失败或取消时回调通知调用方 5、支持关联I/O操作与对应的处理程序。 不同协议、不通的阻塞类型的连接都有不同的Channel类型与之对应 下面是一些常用的Channel类型： NioSocketChannel 异步的客户端TCP Socket连接 NioServerSocketChannel 异步的服务器端TCP Socket连接 NioDatagramChannel 异步的UDP连接 NioSctpChannel 异步的客户端Sctp连接 NioSctpServerChannel 异步的Sctp服务器端连接 这些通道涵盖了UDP和TCP网络IO以及文件IO Selector Netty基于Selector对象实现I/O多路复用，通过Selector一个线程可以监听多个连接的Channel事件。 当向一个Selector中注册Channel后，Selector内部的机制就可以自动不断地查询（Select）这些注册的Channel是否有已就绪的I/O事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多 个 Channel 。 NioEventLoop NioEventLoop中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用NioEventLoop 的 run 方 法，执行 I/O 任务和非 I/O 任务： I/O任务，即selectionKey中ready的事件，如accept、connect、read、write等，由 processSelectedKeys 方 法触发。 非 IO 任务，添加到 taskQueue 中的任务，如 register0、bind0 等任务，由 runAllTasks 方法触发。 NioEventLoopGroup NioEventLoopGroup，主要管理eventLoop的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程（NioEventLoop)负责处理多个Channel上的事件，而一个Channel只对应于一个线程 ChannelHandler ChannelHandler是一个接口，处理I/O事件或拦截I/O操作，并将其转发到其ChannelPipeline(业务处理链)中的下一个处理程序 ChannelHandler本身并没有提供很多方法，因为这个接口有许多的方法需要实现，方便使用期间，可以继承它的子类： ChannelInboundHandler 用于处理入站I/O事件 ChannelOutboundHandler 用于处理出站I/O操作 或者使用以下适配器类 ChannelInboundHandlerAdapter 用于处理入站I/O事件 ChannlOutboundHandler 用于处理出站I/O操作 ChannelHandlerContext 保存Channel相关的上下文信息，同时关联一个ChannelHandler对象 ChannelPipline ​ 保存ChannelHandler的List,用于处理或拦截Channel的入站事件和出站操作 ChannelPipeline实现了一中高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，事件的处理方式 在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下： ​ 一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组 成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。 read事件(入站事件)和write事件(出站事件)在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler，出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰 Netty架构图 ByteBuf ​ 从结构上来说，ByteBuf 由一串字节数组构成。数组中每个字节用来存放信息。 ByteBuf 提供了两个索引，一个用于读取数据，一个用于写入数据。这两个索引通过在字节数组中移动，来定 位需要读或者写信息的位置。 当从 ByteBuf 读取时，它的 readerIndex（读索引）将会根据读取的字节数递增。 同样，当写 ByteBuf 时，它的 writerIndex 也会根据写入的字节数进行递增。 ​ 需要注意的是极限的情况是 readerIndex 刚好读到了 writerIndex 写入的地方。 如果 readerIndex 超过了 writerIndex 的时候，Netty 会抛出 IndexOutOf-BoundsException 异常 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041public class TestByteBuf &#123; public static void main(String[] args) &#123; // 创建byteBuf对象，该对象内部包含一个字符数组byte[10] // 通过readerindex和writeindex和capacity，将buffer分成三个区域， // 已经读取的区域: [0,readerindex) 注意开闭区间 // 可读取的区域: [readerindex,writeindex) // 可写的区域: [writerindex,capacity） ByteBuf buffer = Unpooled.buffer(10); System.out.println(buffer); for (int i = 0; i &lt; 7; i++) &#123; buffer.writeByte(i); &#125; System.out.println(buffer); for (int i = 0; i &lt; 5; i++) &#123; System.out.println(buffer.getByte(i)); &#125; System.out.println(buffer); for (int i = 0; i &lt; 5; i++) &#123; System.out.println(buffer.readByte()); &#125; System.out.println(buffer); System.out.println(&quot;----------------&quot;); ByteBuf byteBuf2 = Unpooled.copiedBuffer(&quot;hello world&quot;, CharsetUtil.UTF_8); if (byteBuf2.hasArray())&#123; byte[] array = byteBuf2.array(); // 转成字符串 System.out.println(new String(array, CharsetUtil.UTF_8)); System.out.println(byteBuf2); System.out.println(byteBuf2.readerIndex()); System.out.println(byteBuf2.writerIndex()); System.out.println(byteBuf2.capacity()); System.out.println(byteBuf2.getByte(0)); // 获取数组0这个位置的字符h的ascii码，h=104 int len = byteBuf2.readableBytes(); // 可读的字节数 System.out.println(&quot;len = &quot; + len); &#125; // 范围读取 CharSequence charSequence = byteBuf2.getCharSequence(0, 6, CharsetUtil.UTF_8); System.out.println(charSequence.toString()); &#125;&#125; Netty编解码 Netty涉及到编解码的组件有Channel、ChannelHandler、ChannelPipe等 ChannelHandler ChannelHandler充当了处理入站和出站数据的应用程序逻辑容器，例如，实现ChannelInboundHandler接口（或 ChannelInboundHandlerAdapter)，你就可以接收入站事件和数据，这些数据随后会被你的应用程序的业务逻辑处理。当你要给连接的客户端发送响应时，也可以从ChannelInboundHandler冲刷数据。你得业务逻辑通常写在一个或者多个ChannelInboundHandler中。ChannelOutboundHandler原理一样，只不过它用来处理出站数据的。 ChannelPipeline ChannelPipeline提供了ChannelHandler链的容器。以客户端应用程序为例，如果事件的运动方向时从客户端到服务端的，那么称为这些事件为出战的，即客户端发送给服务端的数据会通过pipeline中的一系列ChannelOutboundHandler(ChannelOutboundHandler)调用是从tail到head方向逐个调用每个handler的逻辑，并被这些handler处理，反之则称为入站的，入站只调用pipeline里的ChannelInboundHandler逻辑（ChannelInboundHandler调用是从head到tail方向逐个调用每个handler的逻辑 所谓的入站出站，是相当于客户端/服务端来说的，即收到消息为入站，消息发送为出站。入站会从head到tail经过一系列处理调用，但是入站只是会调用继承ChannelInboundHandler的逻辑，出站是从tail到尾进行处理调用，只会调用ChannelOutboundHandler 编码解码器 ​ 当你通过Netty发送或者接收一个消息时，就将会发生一次数据转换。入站消息会被解码：从字节转换为另一种格式（比如java对 象）；如果是出站消息，它会被编码成字节。 ​ Netty提供了一系列的编码解码器，他们都实现了ChannelInboundHadnler或者ChannelOutboundHandler接口。在这些类中，channelRead方法已经被重写了。 ​ 以入站为例，对于每个从入站Channel读取的消息，这个方法会被调用。随后，它将调用由已知解码器 所提供的decode()方法进行解码，并将已经解码的字节转发给ChannelPipeline中的下一个ChannelInboundHandler。Netty提供了很多编解码器，比如编解码字符串的StringEncoder和StringDecoder，编解码对象的ObjectEncoder和ObjectDecoder 等。 如果要实现高效的编解码可以用protobuf，但是protobuf需要维护大量的proto文件比较麻烦，现在一般可以使用protostuff。 protostuff是一个基于protobuf实现的序列化方法，它较于protobuf最明显的好处是，在几乎不损耗性能的情况下做到了不用我们 写.proto文件来实现序列化 maven坐标如下： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff‐api&lt;/artifactId&gt; &lt;version&gt;1.0.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff‐core&lt;/artifactId&gt; &lt;version&gt;1.0.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff‐runtime&lt;/artifactId&gt; &lt;version&gt;1.0.10&lt;/version&gt;&lt;/dependency&gt; Netty粘包拆包 TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。 ​ 面向流的通信是无消息保护边界的。 ​ 如下图：client发送两个数据包D1和D2，但是server端可能会收到如下几种情况的数据 解决方案 1、消息定长度，数据传输的大小固定长度，例如每段长度固定100字节，不够空位补齐 2、在数据包尾部添加特殊分隔符，比如下划线等。前提是消息本体不能带分隔符 3、发送长度：发送每条数据的时候，将数据的长度一并发送。比如可以选取每条数据的前四位去记录长度，接受处理时可以根据长度判定开始和结束 Netty提供了多个解码器，可以进行分包的操作 LineBasedFrameDecoder (回车换行分包) DelimiterBasedFrameDecoder (特殊分隔符分包) FixedLengthFrameDecoder (固定长度报文分包) Netty心跳检测机制​ 在Netty中，实现心跳机制的关键是IDleStateHandler。 123456789101112........childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel)&#123; socketChannel.pipeline().addLast(new StringDecoder()); socketChannel.pipeline().addLast(new StringEncoder()); // IdleStateEvent的readerIdleTime参数指定超过3秒还没收到客户端的连接，会触发 // IdleStateEvent事件并且交给下一个handler处理，下一个handler必须实现userEventTriggered方法处理对应事件 socketChannel.pipeline().addLast(new IdleStateHandler(3,0,0, TimeUnit.SECONDS)); socketChannel.pipeline().addLast(new HeartBeatServerHandler()); &#125;&#125;); 构造函数 12345678910111213141516171819202122232425262728293031323334353637383940// 读超时时间；写超时时间；所有的超时时间；时间单位public IdleStateHandler(long readerIdleTime, long writerIdleTime, long allIdleTime, TimeUnit unit) &#123; this(false, readerIdleTime, writerIdleTime, allIdleTime, unit);&#125;public IdleStateHandler(boolean observeOutput, long readerIdleTime, long writerIdleTime, long allIdleTime, TimeUnit unit) &#123; this.writeListener = new ChannelFutureListener() &#123; public void operationComplete(ChannelFuture future) throws Exception &#123; IdleStateHandler.this.lastWriteTime = IdleStateHandler.this.ticksInNanos(); IdleStateHandler.this.firstWriterIdleEvent = IdleStateHandler.this.firstAllIdleEvent=true; &#125; &#125;; this.firstReaderIdleEvent = true; this.firstWriterIdleEvent = true; this.firstAllIdleEvent = true; if (unit == null) &#123; throw new NullPointerException(&quot;unit&quot;); &#125; else &#123; this.observeOutput = observeOutput; if (readerIdleTime &lt;= 0L) &#123; this.readerIdleTimeNanos = 0L; &#125; else &#123; // 赋值 this.readerIdleTimeNanos = Math.max(unit.toNanos(readerIdleTime), MIN_TIMEOUT_NANOS); &#125; if (writerIdleTime &lt;= 0L) &#123; this.writerIdleTimeNanos = 0L; // 赋值 &#125; else &#123; this.writerIdleTimeNanos = Math.max(unit.toNanos(writerIdleTime), MIN_TIMEOUT_NANOS); &#125; if (allIdleTime &lt;= 0L) &#123; this.allIdleTimeNanos = 0L; // 赋值 &#125; else &#123; this.allIdleTimeNanos = Math.max(unit.toNanos(allIdleTime), MIN_TIMEOUT_NANOS); &#125; &#125;&#125; channelActive 1234public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; this.initialize(ctx); super.channelActive(ctx);&#125; initialize 1234567891011121314151617181920212223private void initialize(ChannelHandlerContext ctx) &#123; switch(this.state) &#123; case 1: case 2: return; default: this.state = 1; this.initOutputChanged(ctx); this.lastReadTime = this.lastWriteTime = this.ticksInNanos(); // lastReadTime,lastWriteTime赋初始值 if (this.readerIdleTimeNanos &gt; 0L) &#123; // readerIdleTimeNanos就是刚构造器里赋的值 this.readerIdleTimeout = this.schedule(ctx, new IdleStateHandler.ReaderIdleTimeoutTask(ctx), this.readerIdleTimeNanos, TimeUnit.NANOSECONDS); &#125; if (this.writerIdleTimeNanos &gt; 0L) &#123; this.writerIdleTimeout = this.schedule(ctx, new IdleStateHandler.WriterIdleTimeoutTask(ctx), this.writerIdleTimeNanos, TimeUnit.NANOSECONDS); &#125; if (this.allIdleTimeNanos &gt; 0L) &#123; this.allIdleTimeout = this.schedule(ctx, new IdleStateHandler.AllIdleTimeoutTask(ctx), this.allIdleTimeNanos, TimeUnit.NANOSECONDS); &#125; &#125;&#125; schedule 123ScheduledFuture&lt;?&gt; schedule(ChannelHandlerContext ctx, Runnable task, long delay, TimeUnit unit) &#123; return ctx.executor().schedule(task, delay, unit);&#125; ChannelHandlerContext ctx是io.netty.channel.DefaultChannelHandlerContext实例，DefaultChannleHandlerContext又继承了AbstractChannelHandlerContext，在AbstractChannelHandlerContext中找到executor()方法，如下 123public EventExecutor executor() &#123; return (EventExecutor)(this.executor == null ? this.channel().eventLoop() : this.executor);&#125; ​ ctx.executor()会返回一个EventExecutor，其类图如下，跟ScheduledThreadPollExecutor定时线程池一样，其都继承或实现自ScheduledExecutorService接口 接着我们去看new IdleStateHandler.ReaderIdleTimeoutTask(ctx)，这个任务里的run方法 123456789101112131415161718192021222324protected void run(ChannelHandlerContext ctx) &#123; long nextDelay = IdleStateHandler.this.readerIdleTimeNanos; if (!IdleStateHandler.this.reading) &#123; // nextDelay = readerIdleTimeNanos - (当前时间 - lastReadTime) // channelReadComplete()方法里，会更新lastReadTime // 其实就是在计算 读的时间间隔是否超过设定的时间 nextDelay -= IdleStateHandler.this.ticksInNanos() - IdleStateHandler.this.lastReadTime; &#125; if (nextDelay &lt;= 0L) &#123; // 小于等于0，即超时了走这里 IdleStateHandler.this.readerIdleTimeout = IdleStateHandler.this.schedule(ctx, this, IdleStateHandler.this.readerIdleTimeNanos, TimeUnit.NANOSECONDS); // 还会提交任务，延迟时间为readerIdleTimeNanos boolean first = IdleStateHandler.this.firstReaderIdleEvent;// 构造函数赋初始值为true IdleStateHandler.this.firstReaderIdleEvent = false; try &#123; IdleStateEvent event = IdleStateHandler.this.newIdleStateEvent(IdleState.READER_IDLE, first); IdleStateHandler.this.channelIdle(ctx, event); &#125; catch (Throwable var6) &#123; ctx.fireExceptionCaught(var6); &#125; &#125; else &#123; IdleStateHandler.this.readerIdleTimeout = IdleStateHandler.this.schedule(ctx, this, nextDelay, TimeUnit.NANOSECONDS); &#125;&#125; 123protected void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) throws Exception &#123; ctx.fireUserEventTriggered(evt); // 会调用到下一个handler&#125; 源码剖析NioEventLoopGroup12NioEventLoopGroup bossGroup = new NioEventLoopGroup(1);NioEventLoopGroup workerGroup = new NioEventLoopGroup(); 构造函数 1234567891011121314151617181920public NioEventLoopGroup() &#123; this(0);&#125;public NioEventLoopGroup(int nThreads) &#123; this(nThreads, (Executor)null);&#125;public NioEventLoopGroup(int nThreads, Executor executor) &#123; this(nThreads, executor, SelectorProvider.provider()); //SelectorProvider.provider()就是前面NIO所说的EPollSelectorProvider,详情看Selector.open()&#125;.......// 无参构造最终会调用这个有参构造 public NioEventLoopGroup(int nThreads, ThreadFactory threadFactory, SelectorProvider selectorProvider, SelectStrategyFactory selectStrategyFactory) &#123; super(nThreads, threadFactory, new Object[]&#123;selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject()&#125;); //调用父类MultithreadEventLoopGroup的构造器 &#125; io.netty.channel.MultithreadEventLoopGroup 123456private static final int DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt(&quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2));protected MultithreadEventLoopGroup(int nThreads, ThreadFactory threadFactory, Object... args) &#123; super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, threadFactory, args); //继续调用父类构造器，nThreads = 0的话，会传入默认的线程数，cpu核数*2&#125; io.netty.util.concurrent.MultithreadEventExecutorGroup 123protected MultithreadEventExecutorGroup(int nThreads, Executor executor, Object... args) &#123; this(nThreads, executor, DefaultEventExecutorChooserFactory.INSTANCE, args);&#125; 最终会调用MultithreadEventExecutorGroup类的这个构造方法 123456789101112131415161718192021222324252627282930protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) &#123; // nThreads = 16，executor = null this.terminatedChildren = new AtomicInteger(); this.terminationFuture = new DefaultPromise(GlobalEventExecutor.INSTANCE); if (nThreads &lt;= 0) &#123; throw new IllegalArgumentException(String.format(&quot;nThreads: %d (expected: &gt; 0)&quot;, nThreads)); &#125; else &#123; if (executor == null) &#123; // 创建一个线程池 executor = new ThreadPerTaskExecutor(this.newDefaultThreadFactory()); &#125; this.children = new EventExecutor[nThreads]; // 创建一个EventExecutor数组，长度为nThreads（16） int j; for(int i = 0; i &lt; nThreads; ++i) &#123; boolean success = false; boolean var18 = false; try &#123; var18 = true; // 循环遍历，对数组中的每个元素赋值；会去new NioEventLoop(...) this.children[i] = this.newChild((Executor)executor, args); success = true; var18 = false; &#125; catch (Exception var19) &#123; throw new IllegalStateException(&quot;failed to create a child event loop&quot;, var19); &#125; finally &#123; ....... &#125;&#125; newChild() io.netty.channel.nio.NioEventLoopGroup#newChild 123protected EventLoop newChild(Executor executor, Object... args) throws Exception &#123; return new NioEventLoop(this, executor, (SelectorProvider)args[0], ((SelectStrategyFactory)args[1]).newSelectStrategy(), (RejectedExecutionHandler)args[2]);&#125; io.netty.channel.nio.NioEventLoop 构造函数 public final class NioEventLoop extends SingleThreadEventLoop 1234567891011121314NioEventLoop(NioEventLoopGroup parent, Executor executor, SelectorProvider selectorProvider, SelectStrategy strategy, RejectedExecutionHandler rejectedExecutionHandler) &#123; super(parent, executor, false, DEFAULT_MAX_PENDING_TASKS, rejectedExecutionHandler); // 调用父类构造器,里面会创建一个阻塞队列taskQueue if (selectorProvider == null) &#123; throw new NullPointerException(&quot;selectorProvider&quot;); &#125; else if (strategy == null) &#123; throw new NullPointerException(&quot;selectStrategy&quot;); &#125; else &#123; this.provider = selectorProvider; NioEventLoop.SelectorTuple selectorTuple = this.openSelector(); // 类比前面NIO的代码，创建selector this.selector = selectorTuple.selector; this.unwrappedSelector = selectorTuple.unwrappedSelector; this.selectStrategy = strategy; &#125;&#125; io.netty.channel.SingleThreadEventLoop 构造函数 public abstract class SingleThreadEventLoop extends SingleThreadEventExecutor implements EventLoop 1234protected SingleThreadEventLoop(EventLoopGroup parent, Executor executor, boolean addTaskWakesUp, int maxPendingTasks, RejectedExecutionHandler rejectedExecutionHandler) &#123; super(parent, executor, addTaskWakesUp, maxPendingTasks, rejectedExecutionHandler); this.tailTasks = this.newTaskQueue(maxPendingTasks);&#125; io.netty.util.concurrent.SingleThreadEventExecutor 构造函数 12345678910111213141516protected SingleThreadEventExecutor(EventExecutorGroup parent, Executor executor, boolean addTaskWakesUp, int maxPendingTasks, RejectedExecutionHandler rejectedHandler) &#123; super(parent); this.threadLock = new Semaphore(0); this.shutdownHooks = new LinkedHashSet(); this.state = 1; this.terminationFuture = new DefaultPromise(GlobalEventExecutor.INSTANCE); this.addTaskWakesUp = addTaskWakesUp; this.maxPendingTasks = Math.max(16, maxPendingTasks); this.executor = ThreadExecutorMap.apply(executor, this); this.taskQueue = this.newTaskQueue(this.maxPendingTasks); // 创建阻塞队列 this.rejectedExecutionHandler = (RejectedExecutionHandler)ObjectUtil.checkNotNull(rejectedHandler, &quot;rejectedHandler&quot;);&#125;protected Queue&lt;Runnable&gt; newTaskQueue(int maxPendingTasks) &#123; return new LinkedBlockingQueue(maxPendingTasks);&#125; ServerBootstrap 12ServerBootstrap serverBootstrap = new ServerBootstrap();serverBootstrap.group(bossGroup,workerGroup)..... ServerBootstrap构造器为空方法，没有做什么逻辑处理 group(bossGroup,workerGroup)1234567891011public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) &#123; super.group(parentGroup); if (childGroup == null) &#123; throw new NullPointerException(&quot;childGroup&quot;); &#125; else if (this.childGroup != null) &#123; throw new IllegalStateException(&quot;childGroup set already&quot;); &#125; else &#123; this.childGroup = childGroup; return this; &#125;&#125; io.netty.bootstrap.AbstractBootstrap#group public abstract class AbstractBootstrap&lt;B extends AbstractBootstrap&lt;B, C&gt;, C extends Channel&gt; implements Cloneable {} 12345678910public B group(EventLoopGroup group) &#123; if (group == null) &#123; throw new NullPointerException(&quot;group&quot;); &#125; else if (this.group != null) &#123; throw new IllegalStateException(&quot;group set already&quot;); &#125; else &#123; this.group = group; // 赋值 return this.self(); &#125;&#125; channel(NioServerSocketChannel.class)1234567public B channel(Class&lt;? extends C&gt; channelClass) &#123; if (channelClass == null) &#123; throw new NullPointerException(&quot;channelClass&quot;); &#125; else &#123; return this.channelFactory((new ReflectiveChannelFactory(channelClass))); &#125;&#125; new ReflectiveChannelFactory(channelClass) 12345678910private final Constructor&lt;? extends T&gt; constructor; // 成员属性public ReflectiveChannelFactory(Class&lt;? extends T&gt; clazz) &#123; ObjectUtil.checkNotNull(clazz, &quot;clazz&quot;); try &#123; this.constructor = clazz.getConstructor(); // 获取传进来class的构造函数，并赋值给成员属性 &#125; catch (NoSuchMethodException var3) &#123; throw new IllegalArgumentException(&quot;Class &quot; + StringUtil.simpleClassName(clazz) + &quot; does not have a public non-arg constructor&quot;, var3); &#125; this.channelFactory((new ReflectiveChannelFactory(channelClass))); channelFactory()方法,就是将channelFactory赋值给成员属性 123456public B channelFactory(ChannelFactory&lt;? extends C&gt; channelFactory) &#123; ....... this.channelFactory = channelFactory; return this.self(); &#125;&#125; option(ChannelOption.SO_BACKLOG,1024)12345678910111213141516171819private final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = new LinkedHashMap(); // 成员属性public &lt;T&gt; B option(ChannelOption&lt;T&gt; option, T value) &#123; if (option == null) &#123; throw new NullPointerException(&quot;option&quot;); &#125; else &#123; if (value == null) &#123; synchronized(this.options) &#123; this.options.remove(option); &#125; &#125; else &#123; synchronized(this.options) &#123; this.options.put(option, value); // 就是把传进来的key,value放进map &#125; &#125; return this.self(); &#125;&#125; childHandler(new ChannelInitializer() {…}handler()是发生在初始化的时候，childHandler()是发生在客户端连接之后 .childHandler(new ChannelInitializer() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { socketChannel.pipeline().addLast(new ….); }}); 12345678910private volatile ChannelHandler childHandler; // 成员属性public ServerBootstrap childHandler(ChannelHandler childHandler) &#123; if (childHandler == null) &#123; throw new NullPointerException(&quot;childHandler&quot;); &#125; else &#123; this.childHandler = childHandler; return this; &#125;&#125; bind ChannelFuture cf = serverBootstrap.bind(ip,port); ServerBootstrap继承AbstractBootstrap，bind是父类AbstractBootstrap的方法 123public ChannelFuture bind(int inetPort) &#123; return this.bind(new InetSocketAddress(inetPort));&#125; 调用重载方法 12345678public ChannelFuture bind(SocketAddress localAddress) &#123; this.validate(); // 参数校验 if (localAddress == null) &#123; throw new NullPointerException(&quot;localAddress&quot;); &#125; else &#123; return this.doBind(localAddress); &#125;&#125; validate方法 校验成员属性是否有值,也就是group()，channel()配置的那些 123456789public B validate() &#123; if (this.group == null) &#123; throw new IllegalStateException(&quot;group not set&quot;); &#125; else if (this.channelFactory == null) &#123; throw new IllegalStateException(&quot;channel or channelFactory not set&quot;); &#125; else &#123; return this.self(); &#125;&#125; this.doBind(localAddress)方法 1234567891011121314151617181920212223242526private ChannelFuture doBind(final SocketAddress localAddress) &#123; final ChannelFuture regFuture = this.initAndRegister(); // ① final Channel channel = regFuture.channel(); if (regFuture.cause() != null) &#123; return regFuture; &#125; else if (regFuture.isDone()) &#123; ChannelPromise promise = channel.newPromise(); doBind0(regFuture, channel, localAddress, promise); // ② return promise; &#125; else &#123; final AbstractBootstrap.PendingRegistrationPromise promise = new AbstractBootstrap.PendingRegistrationPromise(channel); regFuture.addListener(new ChannelFutureListener() &#123; public void operationComplete(ChannelFuture future) throws Exception &#123; Throwable cause = future.cause(); if (cause != null) &#123; promise.setFailure(cause); &#125; else &#123; promise.registered(); AbstractBootstrap.doBind0(regFuture, channel, localAddress, promise); &#125; &#125; &#125;); return promise; &#125;&#125; this.initAndRegister()方法1234567891011121314151617181920212223final ChannelFuture initAndRegister() &#123; Channel channel = null; try &#123; // channelFactory就是channel方法赋值的 ReflectiveChannelFactory // ReflectiveChannelFactory的newChannel()方法主要是: return (Channel)this.constructor.newInstance(); // 如前文所说，constructor也是channel方法赋值的，我们传进来的是NioServerSocketChannel.class channel = this.channelFactory.newChannel(); this.init(channel); &#125; catch (Throwable var3) &#123; ...... &#125; ChannelFuture regFuture = this.config().group().register(channel); // 将NioServerSocketChannel注册 if (regFuture.cause() != null) &#123; if (channel.isRegistered()) &#123; channel.close(); &#125; else &#123; channel.unsafe().closeForcibly(); &#125; &#125; return regFuture;&#125; NioServerSocketChannel的无参构造：12345private static final SelectorProvider DEFAULT_SELECTOR_PROVIDER = SelectorProvider.provider(); // 成员属性public NioServerSocketChannel() &#123; this(newSocket(DEFAULT_SELECTOR_PROVIDER));&#125; 先看newSocket()方法 1234567private static java.nio.channels.ServerSocketChannel newSocket(SelectorProvider provider) &#123; try &#123; return provider.openServerSocketChannel(); // 类比文章开头的NIO代码，几乎一模一样 会去创建ServerSocketChannel对象 &#125; catch (IOException var2) &#123; throw new ChannelException(&quot;Failed to open a server socket.&quot;, var2); &#125;&#125; this(newSocket(DEFAULT_SELECTOR_PROVIDER));调用重载的构造器 1234public NioServerSocketChannel(java.nio.channels.ServerSocketChannel channel) &#123; super((Channel)null, channel, SelectionKey.OP_ACCEPT); // SelectionKey.OP_ACCEPT 连接事件 this.config = new NioServerSocketChannel.NioServerSocketChannelConfig(this, this.javaChannel().socket());&#125; 父类构造器 12345678910111213141516171819202122232425262728293031323334353637protected AbstractNioMessageChannel(Channel parent, SelectableChannel ch, int readInterestOp) &#123; super(parent, ch, readInterestOp); // ① 继续调用父类构造器，如下&#125;protected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) &#123; super(parent); // ② 继续往上调用,代码如下 this.ch = ch; this.readInterestOp = readInterestOp; try &#123; ch.configureBlocking(false); // 设置ServerSocketChannel为非阻塞 &#125; catch (IOException var7) &#123; ...... &#125;&#125;protected AbstractChannel(Channel parent) &#123; // ③ this.parent = parent; this.id = this.newId(); this.unsafe = this.newUnsafe(); this.pipeline = this.newChannelPipeline(); // 创造pipeline&#125;protected DefaultChannelPipeline newChannelPipeline() &#123; // ④ return new DefaultChannelPipeline(this);&#125;protected DefaultChannelPipeline(Channel channel) &#123; // ⑤ this.channel = (Channel)ObjectUtil.checkNotNull(channel, &quot;channel&quot;); this.succeededFuture = new SucceededChannelFuture(channel, (EventExecutor)null); this.voidPromise = new VoidChannelPromise(channel, true); // 创建尾部节点，TailContext与HeadContext都间接实现了ChannelHandlerContext，AbstractChannelHandlerContext实现了它 // class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler this.tail = new DefaultChannelPipeline.TailContext(this); this.head = new DefaultChannelPipeline.HeadContext(this); // 创建头部节点 this.head.next = this.tail; // 首尾互指 this.tail.prev = this.head;&#125; ch.configureBlocking(false); 即前面NIO代码的如下地方： this.init(channel); this.init()是一个抽象方法，ServerBootstrap中对应实现如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = new LinkedHashMap(); // AbstractBootstrap中的成员属性void init(Channel channel) throws Exception &#123; //channel ——&gt; NioServerSocketChannel实例 // options0（）是父类AbstractBootstrap方法，具体逻辑就是 return this.options; // this.options是前文所说的option()方法赋的值 Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = this.options0(); synchronized(options) &#123; // 会去遍历options,给nioServerSocketChannel里的成员属性ServerSocketChannelConfig赋值 setChannelOptions(channel, options, logger); ， &#125; Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = this.attrs0(); synchronized(attrs) &#123; Iterator var5 = attrs.entrySet().iterator(); while(true) &#123; if (!var5.hasNext()) &#123; break; &#125; Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e = (Entry)var5.next(); AttributeKey&lt;Object&gt; key = (AttributeKey)e.getKey(); channel.attr(key).set(e.getValue()); &#125; &#125; ChannelPipeline p = channel.pipeline(); // 拿到NioServerSocketChannel中的pipeline final EventLoopGroup currentChildGroup = this.childGroup; final ChannelHandler currentChildHandler = this.childHandler; final Entry[] currentChildOptions; synchronized(this.childOptions) &#123; currentChildOptions = (Entry[])this.childOptions.entrySet().toArray(newOptionArray(0)); &#125; final Entry[] currentChildAttrs; synchronized(this.childAttrs) &#123; currentChildAttrs = (Entry[])this.childAttrs.entrySet().toArray(newAttrArray(0)); &#125; p.addLast(new ChannelHandler[]&#123;new ChannelInitializer&lt;Channel&gt;() &#123; // 往pipeline里添加一个handler public void initChannel(final Channel ch) throws Exception &#123; final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = ServerBootstrap.this.config.handler(); if (handler != null) &#123; pipeline.addLast(new ChannelHandler[]&#123;handler&#125;); &#125; ch.eventLoop().execute(new Runnable() &#123; public void run() &#123; pipeline.addLast(new ChannelHandler[]&#123;new ServerBootstrap.ServerBootstrapAcceptor(ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)&#125;); &#125; &#125;); &#125; &#125;&#125;);&#125; this.config().group().register(channel); this.config()返回一个ServerBootstrapConfig对象 1public abstract AbstractBootstrapConfig&lt;B, C&gt; config(); //抽象方法 ServerBootstarp中的实现如下 12345private final ServerBootstrapConfig config = new ServerBootstrapConfig(this); // 成员属性public final ServerBootstrapConfig config() &#123; return this.config;&#125; ServerBootstrapConfig的构造方法如下： 123456789101112ServerBootstrapConfig(ServerBootstrap bootstrap) &#123; super(bootstrap); // 往上调用&#125;public abstract class AbstractBootstrapConfig&lt;B extends AbstractBootstrap&lt;B, C&gt;, C extends Channel&gt; &#123; protected final B bootstrap; protected AbstractBootstrapConfig(B bootstrap) &#123; // 将ServerBootstarp赋值给成员属性 this.bootstrap = (AbstractBootstrap)ObjectUtil.checkNotNull(bootstrap, &quot;bootstrap&quot;); &#125;&#125; group()方法 123public final EventLoopGroup group() &#123; return this.bootstrap.group(); // this.bootstrap就是ServerBootstrap实例&#125; 12345volatile EventLoopGroup group; // 成员属性；具体在前文所说的ServerBootstrap里的group方法里赋的值public final EventLoopGroup group() &#123; return this.group; // 就是NioEventLoopGroup，准确来说是boss线程组。&#125; register(channel)方法 ​ 前面的this.config().group()会返回NioEventLoopGroup，register是抽象方法；而NioEventLoopGroup继承MultithreadEventLoopGroup，我们看MultithreadEventLoopGroup里的实现 1234567public ChannelFuture register(Channel channel) &#123; // channel ——&gt; NioServerSocketChannel return this.next().register(channel);&#125;public EventLoop next() &#123; return (EventLoop)super.next(); // 拿一个NioEventLoop线程，前面所说会创建(默认长度为16的)children数组，并循环遍历对每一个赋值&#125; ​ 接着继续跟进返回的nioEventLoop的register方法，NioEventLoop继承SingleThreadEventLoop，所以我们看它里面的实现 io.netty.channel.SingleThreadEventLoop#register(io.netty.channel.Channel) 123public ChannelFuture register(Channel channel) &#123; return this.register((ChannelPromise)(new DefaultChannelPromise(channel, this)));&#125; 调用重载方法 12345public ChannelFuture register(ChannelPromise promise) &#123; ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); // this ————&gt; NioEventLoop return promise;&#125; io.netty.channel.AbstractChannel.AbstractUnsafe#register 123456789101112131415161718192021public final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; if (eventLoop == null) &#123; ...... &#125; else &#123; AbstractChannel.this.eventLoop = eventLoop; // 将传进来的NioEventLoop赋值给成员属性 if (eventLoop.inEventLoop()) &#123; // 判断当前线程是否是NioEventLoop中的线程 this.register0(promise); &#125; else &#123; try &#123; eventLoop.execute(new Runnable() &#123; public void run() &#123; AbstractUnsafe.this.register0(promise); &#125; &#125;); &#125; catch (Throwable var4) &#123; ...... &#125; &#125; &#125;&#125; 先看excute方法做了什么 io.netty.util.concurrent.SingleThreadEventExecutor#execute 123456789101112131415161718192021222324252627282930public void execute(Runnable task) &#123; if (task == null) &#123; throw new NullPointerException(&quot;task&quot;); &#125; else &#123; boolean inEventLoop = this.inEventLoop(); this.addTask(task); // 将任务添加到taskQueue队列 if (!inEventLoop) &#123; this.startThread(); if (this.isShutdown()) &#123; boolean reject = false; try &#123; if (this.removeTask(task)) &#123; reject = true; &#125; &#125; catch (UnsupportedOperationException var5) &#123; &#125; if (reject) &#123; reject(); &#125; &#125; &#125; if (!this.addTaskWakesUp &amp;&amp; this.wakesUpForTask(task)) &#123; this.wakeup(inEventLoop); &#125; &#125;&#125; 123456789101112131415private void doStartThread() &#123; this.executor.execute(new Runnable() &#123; public void run() &#123; .... label1907: &#123; try &#123; var112 = true; SingleThreadEventExecutor.this.run(); // 调用NioEventLoop的run方法 success = true; var112 = false; break label1907; &#125; catch (Throwable var119) &#123; ....... &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162protected void run() &#123; while(true) &#123; while(true) &#123; while(true) &#123; try &#123; try &#123; switch(this.selectStrategy.calculateStrategy(this.selectNowSupplier, this.hasTasks())) &#123; case -3: case -1: // select()，类比NIO代码 // 方法里面会去调用 selector.select(timeoutMillis);超时等待 this.select(this.wakenUp.getAndSet(false)); if (this.wakenUp.get()) &#123; this.selector.wakeup(); &#125; break; case -2: continue; &#125; &#125; catch (IOException var23) &#123; this.rebuildSelector0(); handleLoopException(var23); continue; &#125; this.cancelledKeys = 0; this.needsToSelectAgain = false; int ioRatio = this.ioRatio; if (ioRatio == 100) &#123; try &#123; this.processSelectedKeys(); // 类比NIO代码,拿到SelectionKey &#125; finally &#123; this.runAllTasks(); // 执行任务 &#125; &#125; else &#123; long ioStartTime = System.nanoTime(); boolean var14 = false; try &#123; var14 = true; this.processSelectedKeys(); var14 = false; &#125; finally &#123; if (var14) &#123; long ioTime = System.nanoTime() - ioStartTime; this.runAllTasks(ioTime * (long)(100 - ioRatio) / (long)ioRatio); &#125; &#125; long ioTime = System.nanoTime() - ioStartTime; this.runAllTasks(ioTime * (long)(100 - ioRatio) / (long)ioRatio); &#125; &#125; catch (Throwable var24) &#123; handleLoopException(var24); &#125; break; &#125; ....... &#125; &#125;&#125; 1234567891011121314151617181920protected boolean runAllTasks() &#123; assert this.inEventLoop(); boolean ranAtLeastOne = false; boolean fetchedAll; do &#123; fetchedAll = this.fetchFromScheduledTaskQueue(); if (this.runAllTasksFrom(this.taskQueue)) &#123; // 从队列中获取任务并执行 ranAtLeastOne = true; &#125; &#125; while(!fetchedAll); if (ranAtLeastOne) &#123; this.lastExecutionTime = ScheduledFutureTask.nanoTime(); &#125; this.afterRunningAllTasks(); return ranAtLeastOne;&#125; 回过头来看，执行任务的逻辑，即刚刚提交任务的run方法 eventLoop.execute(new Runnable() { public void run() { AbstractUnsafe.this.register0(promise); }}); 123456789101112131415161718192021222324252627private void register0(ChannelPromise promise) &#123; try &#123; if (!promise.setUncancellable() || !this.ensureOpen(promise)) &#123; return; &#125; boolean firstRegistration = this.neverRegistered; AbstractChannel.this.doRegister(); // 注册 this.neverRegistered = false; AbstractChannel.this.registered = true; AbstractChannel.this.pipeline.invokeHandlerAddedIfNeeded(); this.safeSetSuccess(promise); AbstractChannel.this.pipeline.fireChannelRegistered(); // 责任链调用pipeline中的handler if (AbstractChannel.this.isActive()) &#123; if (firstRegistration) &#123; AbstractChannel.this.pipeline.fireChannelActive(); &#125; else if (AbstractChannel.this.config().isAutoRead()) &#123; this.beginRead(); &#125; &#125; &#125; catch (Throwable var3) &#123; this.closeForcibly(); AbstractChannel.this.closeFuture.setClosed(); this.safeSetFailure(promise, var3); &#125;&#125; 12345678910111213141516protected void doRegister() throws Exception &#123; boolean selected = false; while(true) &#123; try &#123; this.selectionKey = this.javaChannel().register(this.eventLoop().unwrappedSelector(), 0, this); // 将serverSocketChannel注册到selector上，并让其对 return; &#125; catch (CancelledKeyException var3) &#123; if (selected) &#123; throw var3; &#125; this.eventLoop().selectNow(); selected = true; &#125; &#125; &#125; 1234public final ChannelPipeline fireChannelRegistered() &#123; AbstractChannelHandlerContext.invokeChannelRegistered(this.head); // 将pipeline中的头节点传进去 return this;&#125; 12345678910111213static void invokeChannelRegistered(final AbstractChannelHandlerContext next) &#123; EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeChannelRegistered(); &#125; else &#123; executor.execute(new Runnable() &#123; // 接着执行 public void run() &#123; next.invokeChannelRegistered(); &#125; &#125;); &#125;&#125; 流程图 无锁串行化​ 大多数场景下，并行多线程处理可以提升系统的并发能力。但是，如果对于共享资源的并发访问处理不当，会带来严重的锁竞争，这最终会导致性能的下降。为了尽可能的避免锁竞争带来的性能损耗，可以通过串行化设计，即消息的处理尽可能在同一个线程内完成，期间不进行线程的切换，这样就避免了多线程竞争和同步锁。NIO的多路复用就是一种无锁串行化的设计思想。为了尽可能提升性能，Netty采用了串行无锁化设计，在IO线程内部进行串行操作，避免多线程竞争导致的性能下降。表面上来看，串行化的设计似乎CPU利用率不高，并发成都不够，但是，通过调整NIO线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程设计相比一个队列-多个工作线程模型性能更优 Netty的NioEventLoop读到消息之后，直接调用ChannelPipeline的fireChannelRead(Object msg)，只要用户不主动切换线程，一直会由NioEventLoop调到用户的handler,期间不进行线程切换 零拷贝（直接内存的使用） 直接内存 直接内存Direct Memory,并不是虚拟机运行时数据区的一部分，某些情况下这部分内存也会被频繁地使用，而且也可能导致OOM,Java里用DirectByteBuffer可以分配一块直接内存(堆外内存) ​ 直接内存申请较慢，但访问效率高。在java虚拟机实现上，本地IO一般会直接操作直接内存（直接内存-&gt;系统调用 -&gt;硬盘/网卡），而非直接内存则需要二次拷贝（堆内存-&gt;直接内存-&gt;系统调用-&gt;硬盘/网卡）。 ​ Netty的接收和发送ByteBuf采用DIRECT BUFFERS ， 使用堆外内存进行Scoket读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的JVM堆内存（HEAP BUFFERS)进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才能写入Socket中，Jvm堆内存的数据是不能写入Socket中的。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。 优点： 不占用堆内存空间，减少GC发生的频率 java虚拟机实现上，本地IO会直接操作直接内存（直接内存——&gt;系统调用——&gt;硬盘/网卡），而非直接内存则需要二次拷贝（堆内存——&gt;直接内存——&gt;系统调用——&gt;硬盘/网卡） 缺点： 初始分配较慢 没有JVM直接帮助管理内存，容易发生内存溢出，为了避免一直没有FULL GC，最终导致直接内存把物理内存耗完。我们可以指定直接内存的最大值，通过-XX:MaxDirectMemorySize来指定，当达到阈值的时候，调用system.gc来进行一次FULL GC，间接把那些没有被使用的直接内存回收掉 ByteBuf内存池设计 ​ 随着JVM虚拟机和JIT即时编译技术的发展，对象的分配和回收是个非常轻量级的工作。但是对于缓冲区Buffer(相当于一个内存块)，情况却稍有不同，特别是对于堆外直接内存的分配和回收，是一件耗时的操作，为了尽量重用缓冲区，Netty提供了基于ByteBuffer内存池的缓冲区重用机制。需要的时候直接从池子里获取ByteBuf使用即可，使用完毕之后就重新放回池子里去。 灵活的TCP参数配置能力​ 合理设置TCP参数在某些场景下对于性能的提升可以起到显著的效果，例如接收缓冲区SO_RCVBUF和发送缓冲区SO_SNDBUF。如果设置不当，对性能的影响是非常大的。通常建议值为128k或者256k Netty在启动辅助类ChannelOption中可以灵活的配置TCP参数，满足不同的用户场景 ByteBuf扩容机制 minNewCapacity：表示用户需要写入的值大小 threshold：阈值，为bytebuf内部设定容量的最大值 maxCapacity：Netty最大能接受的容量大小，一般为int的最大值","categories":[{"name":"netty","slug":"netty","permalink":"http://c89757.gitee.io/colinstar/categories/netty/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://c89757.gitee.io/colinstar/tags/nio/"},{"name":"netty","slug":"netty","permalink":"http://c89757.gitee.io/colinstar/tags/netty/"}]},{"title":"关于count(*)","slug":"关于count()","date":"2021-12-22T12:09:37.000Z","updated":"2021-12-22T13:01:32.776Z","comments":true,"path":"2021/12/22/关于count()/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/22/%E5%85%B3%E4%BA%8Ecount()/","excerpt":"","text":"count(*)的不同实现方式 在 msyql 引擎中，count（*）有不同的实现方式 MyISAM引擎把一个表的总行数存在了磁盘上,因此执行count(*)的时候会直接返回这个数，效率很高 而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累计计数 当然，这里说的是不加where条件的count(*)，如果加了条件，MyISAM表也不能返回这么快的。 为什么InnoDB不像MyISAM一样，也把数字存起来呢？ ​ 因为即使在同一个时刻的多个子查询，由于多版本并发控制（MVCC）的原因，而InnoDB表 应该返回多少行 也是不确定的。 比如现在某表中有1000条数据 会话A去执行select(*) 会话B开启事务，新增一条数据，再执行select * 会话A和会话B在同一时刻执行，那么他们返回的总行数是不一样的，A返回1000，而B返回1001 这和InnoDB的事务有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是MVCC来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB会把数据一行一行的读出来依次判断，可见的行才能够计算“基于这个查询”的表的总行数 ​ MySQL在执行 count(*)操作的时候还是做了优化的。 InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是 主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树 得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑 正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一 解决方案 如果一个页面需要经常查询显示某表的总数，应该如何去做呢？ 我们应该自己去计数 用缓存系统保存计数​ 可以用Redis去记录这个表的总行数。每插入一行Redis计数就加1，每删除一行Redis计数就减1。 可能存在的问题： 1、Redis可能会丢失数据，如果我们刚在表里插入了一行数据，Redis中的值也进行了自增，然后Redis宕机了，还没来得及进行持久化，导致数据的丢失； （我们可以在Redis宕机后，手动select(*)查询总行数写回Redis) 2、Redis和MySql存在分布式事务问题； 比如某个场景下，我们需要查询显示总数，并且还要显示最近操作的100条记录。那我们就需要先从Redis里面取出计数，再去表里取数据记录 可能存在的问题，查到的100行里面没有新增的数据，但Redis的计数已经加1 另一种是，查到的100行有新增的数，但是Redis的计数还没加1 产生的原因就是，无法保证提交数据库事务的同时写入Redis， 在数据库保存计数​ 用一张表去记录总数，可以避免上述问题，因此事务的可见性，我们插入数据和修改表中记录的行数都是在方法执行完后统一提交的事务，事务还未提交时，对其他线程是不可见的 从并发系统性能的角度看，应该先插数据表，还是先更新计数表呢？ 更新计数表会涉及到行锁的竞争，先插入再更新能最大程度的减少了事务之间的锁等待，提高并发度（事务开启后，更新操作放到最后，减少锁等待时间的影响） 不同count的用法count(*)、count(id)、count(字段)、count(1)的用法的性能，有哪些差别呢。 基于InnoDB引擎 count（）是一个聚合函数，对于返回的结果集，一行一行的判断，如果count函数的参数不是null,就会累计值加1，否则不加。 所以count(*),count(id),count(字段),count(1)都返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里，参数“字段”不为null的总个数 对于count(id)来说。InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层，server层拿到id后，判断是不可能为空的，就按行累加 对于count(1)来说。InnoDB引擎遍历整张表，但是不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加 count(*)执行的要比count(id)快，因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作 对于count(字段)来说。 如果这个字段是定义为not null的话，一行行的从记录里面读取出这个字段，判断不能为null,按行累加； 如果这个字段允许为空，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加 对于count(*)来说。并不会把全部字段取出来，而是专门做了优化，不取值，count(*)肯定不是null,按行累加 按照效率排序的话，count(字段) &lt; count(id) &lt; count(1) ≈ count(*)","categories":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/tags/mysql/"}]},{"title":"Sychronized关键字-monitorenter与monitorexit","slug":"Sychronized关键字-monitorenter与monitorexit","date":"2021-12-17T11:19:25.000Z","updated":"2022-01-15T15:13:50.736Z","comments":true,"path":"2021/12/17/Sychronized关键字-monitorenter与monitorexit/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/17/Sychronized%E5%85%B3%E9%94%AE%E5%AD%97-monitorenter%E4%B8%8Emonitorexit/","excerpt":"","text":"每个对象都有一个Monitor与之关联，当Monitor被持有后，它将处于锁定状态。Synchronized在JVM里的实现都是 基于进入和退出Monitor对象来实现方法同步和代码块同步，都可以通过成对的MonitorEnter和MonitorExit指令来实现。 12345public void method() &#123; synchronized(this) &#123; System.out.println(&quot;hello world&quot;); &#125; &#125; 经过javap解析后 1234567891011121314151617181920212223public void method(); Code: 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String hello world 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit 14: goto 22 17: astore_2 18: aload_1 19: monitorexit 20: aload_2 21: athrow 22: return Exception table: from to target type 4 14 17 any 17 20 17 any 此处会发现有一个monitorenter，却有两个monitorexit；这是JVM的补偿机制，保证你的同步代码块中出现异常，能正常释放锁 如字节码行号4-13可能会出现异常，则会走17进行异常处理，在此处进行锁的释放","categories":[{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/categories/JUC/"}],"tags":[{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/tags/JUC/"},{"name":"多线程","slug":"多线程","permalink":"http://c89757.gitee.io/colinstar/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"MySql性能调优","slug":"MySql性能调优","date":"2021-12-16T15:08:30.000Z","updated":"2021-12-17T11:47:30.089Z","comments":true,"path":"2021/12/16/MySql性能调优/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/16/MySql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","excerpt":"","text":"啥也没有，只是为了样式展示","categories":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/tags/mysql/"}]},{"title":"gitee+hexo搭建个人博客","slug":"gitee+hexo搭建个人博客","date":"2021-12-16T12:42:22.000Z","updated":"2021-12-17T11:49:04.429Z","comments":true,"path":"2021/12/16/gitee+hexo搭建个人博客/","link":"","permalink":"http://c89757.gitee.io/colinstar/2021/12/16/gitee+hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"概述事前准备： 先创建一个仓库，同时在仓库根目录下创建index.html (gitee官网这样说的,没试过缺少这个文件会怎样) 安装Hexo所需环境 node.js git 自行进行下载与安装 安装1234567891011# 安装hexonpm install -g hexo# 创建文件夹,用来存储你博客内容hexo init test# cd到创建的目录,执行npm install# 开启hexo服务hexo s 访问http://localhost:4000；没有问题的话就会显示他的默认页面 修改配置关联git仓库，在你创建的目录下找到 config.yml文件（例如此处我的是test/_config.yml） 打开添加如下配置 1234deploy: type: &#x27;git&#x27; repository: https://gitee.com/xxx/xxxx # 你的仓库地址 branch: master # 你的仓库分支 生成静态页面 1hexo g #或者 hexo generate 123456# 此时若出现如下报错：ERROR Local hexo not found in ~/blogERROR Try runing: &#x27;npm install hexo --save&#x27;# 则执行命令：npm install hexo --save 将生成的页面提交到仓库 1hexo d #或者hexo deploy 若执行命令hexo deploy报错：无法连接git或找不到git，则执行如下命令来安装hexo-deployer-git： 1npm install hexo-deployer-git --save 发布文章进入到你创建的“text”目录，新建文章，执行 1hexo new &quot;blog&quot; 此时在test/source/_posts下，会新建一个名为“blog.md”的文件，利用相关markdown编辑器就能编写你的博客啦!(我这里用的typore) 123hexo g # 生成静态页面hexo d # 部署到gitee hexo有许多主题，默认生成的主题都是landscape，你也可以去主题官网寻找自己喜欢的主题 例如主题pure 1234567891011git clone https://github.com/cofess/hexo-theme-pure.git themes/pure#修改test目录下_config.yml里theme的名称,将landscape修改为pure即可 hexo clean#清除缓存文件 (db.json) 和静态文件 (public)hexo g#生成缓存和静态文件hexo d #重新部署到服务器","categories":[{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/categories/others/"}],"tags":[{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/tags/others/"}]}],"categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://c89757.gitee.io/colinstar/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/categories/JUC/"},{"name":"netty","slug":"netty","permalink":"http://c89757.gitee.io/colinstar/categories/netty/"},{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/categories/mysql/"},{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/categories/others/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://c89757.gitee.io/colinstar/tags/maven/"},{"name":"堆","slug":"堆","permalink":"http://c89757.gitee.io/colinstar/tags/%E5%A0%86/"},{"name":"排序算法","slug":"排序算法","permalink":"http://c89757.gitee.io/colinstar/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"JUC","slug":"JUC","permalink":"http://c89757.gitee.io/colinstar/tags/JUC/"},{"name":"多线程","slug":"多线程","permalink":"http://c89757.gitee.io/colinstar/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"nio","slug":"nio","permalink":"http://c89757.gitee.io/colinstar/tags/nio/"},{"name":"netty","slug":"netty","permalink":"http://c89757.gitee.io/colinstar/tags/netty/"},{"name":"mysql","slug":"mysql","permalink":"http://c89757.gitee.io/colinstar/tags/mysql/"},{"name":"others","slug":"others","permalink":"http://c89757.gitee.io/colinstar/tags/others/"}]}